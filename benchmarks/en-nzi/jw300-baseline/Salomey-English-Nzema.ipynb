{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "starter_notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Igc5itf-xMGj"
      },
      "source": [
        "# Masakhane - Machine Translation for African Languages (Using JoeyNMT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x4fXCKCf36IK"
      },
      "source": [
        "## Note before beginning:\n",
        "### - The idea is that you should be able to make minimal changes to this in order to get SOME result for your own translation corpus. \n",
        "\n",
        "### - The tl;dr: Go to the **\"TODO\"** comments which will tell you what to update to get up and running\n",
        "\n",
        "### - If you actually want to have a clue what you're doing, read the text and peek at the links\n",
        "\n",
        "### - With 100 epochs, it should take around 7 hours to run in Google Colab\n",
        "\n",
        "### - Once you've gotten a result for your language, please attach and email your notebook that generated it to masakhanetranslation@gmail.com\n",
        "\n",
        "### - If you care enough and get a chance, doing a brief background on your language would be amazing. See examples in  [(Martinus, 2019)](https://arxiv.org/abs/1906.05685)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l929HimrxS0a"
      },
      "source": [
        "## Retrieve your data & make a parallel corpus\n",
        "\n",
        "If you are wanting to use the JW300 data referenced on the Masakhane website or in our GitHub repo, you can use `opus-tools` to convert the data into a convenient format. `opus_read` from that package provides a convenient tool for reading the native aligned XML files and to convert them to TMX format. The tool can also be used to fetch relevant files from OPUS on the fly and to filter the data as necessary. [Read the documentation](https://pypi.org/project/opustools-pkg/) for more details.\n",
        "\n",
        "Once you have your corpus files in TMX format (an xml structure which will include the sentences in your target language and your source language in a single file), we recommend reading them into a pandas dataframe. Thankfully, Jade wrote a silly `tmx2dataframe` package which converts your tmx file to a pandas dataframe. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oGRmDELn7Az0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "fb69ef39-6d01-4b27-981e-e841871a5271"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Cn3tgQLzUxwn",
        "colab": {}
      },
      "source": [
        "# TODO: Set your source and target languages. Keep in mind, these traditionally use language codes as found here:\n",
        "# These will also become the suffix's of all vocab and corpus files used throughout\n",
        "import os\n",
        "source_language = \"en\"\n",
        "target_language = \"nzi\" \n",
        "lc = False  # If True, lowercase the data.\n",
        "seed = 42  # Random seed for shuffling.\n",
        "tag = \"baseline\" # Give a unique name to your folder - this is to ensure you don't rewrite any models you've already submitted\n",
        "\n",
        "os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\n",
        "os.environ[\"tgt\"] = target_language\n",
        "os.environ[\"tag\"] = tag\n",
        "\n",
        "# This will save it to a folder in our gdrive instead!\n",
        "!mkdir -p \"/content/drive/My Drive/masakhane/$src-$tgt-$tag\"\n",
        "os.environ[\"gdrive_path\"] = \"/content/drive/My Drive/masakhane/%s-%s-%s\" % (source_language, target_language, tag)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kBSgJHEw7Nvx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "52927a88-e700-49b0-8071-7198757afb7f"
      },
      "source": [
        "!echo $gdrive_path"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/masakhane/en-nzi-baseline\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gA75Fs9ys8Y9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "632abb94-6944-4ef2-ae92-7c4ac9ce84db"
      },
      "source": [
        "# Install opus-tools\n",
        "! pip install opustools-pkg"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting opustools-pkg\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/9f/e829a0cceccc603450cd18e1ff80807b6237a88d9a8df2c0bb320796e900/opustools_pkg-0.0.52-py3-none-any.whl (80kB)\n",
            "\r\u001b[K     |████                            | 10kB 21.9MB/s eta 0:00:01\r\u001b[K     |████████                        | 20kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 30kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 40kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 51kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 61kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 71kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 5.2MB/s \n",
            "\u001b[?25hInstalling collected packages: opustools-pkg\n",
            "Successfully installed opustools-pkg-0.0.52\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xq-tDZVks7ZD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "375fb7ed-e597-4a84-d106-ccbb97738334"
      },
      "source": [
        "# Downloading our corpus\n",
        "! opus_read -d JW300 -s $src -t $tgt -wm moses -w jw300.$src jw300.$tgt -q\n",
        "\n",
        "# extract the corpus file\n",
        "! gunzip JW300_latest_xml_$src-$tgt.xml.gz"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Alignment file /proj/nlpl/data/OPUS/JW300/latest/xml/en-nzi.xml.gz not found. The following files are available for downloading:\n",
            "\n",
            " 924 KB https://object.pouta.csc.fi/OPUS-JW300/v1/xml/en-nzi.xml.gz\n",
            " 263 MB https://object.pouta.csc.fi/OPUS-JW300/v1/xml/en.zip\n",
            "  10 MB https://object.pouta.csc.fi/OPUS-JW300/v1/xml/nzi.zip\n",
            "\n",
            " 274 MB Total size\n",
            "./JW300_latest_xml_en-nzi.xml.gz ... 100% of 924 KB\n",
            "./JW300_latest_xml_en.zip ... 100% of 263 MB\n",
            "./JW300_latest_xml_nzi.zip ... 100% of 10 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n48GDRnP8y2G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "b3337e78-0ec4-4a1d-c714-028386954e76"
      },
      "source": [
        "# Download the global test set.\n",
        "! wget https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-any.en\n",
        "  \n",
        "# And the specific test set for this language pair.\n",
        "os.environ[\"trg\"] = target_language \n",
        "os.environ[\"src\"] = source_language \n",
        "\n",
        "! wget https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-$trg.en \n",
        "! mv test.en-$trg.en test.en\n",
        "! wget https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-$trg.$trg \n",
        "! mv test.en-$trg.$trg test.$trg"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-03 19:50:55--  https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-any.en\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 277791 (271K) [text/plain]\n",
            "Saving to: ‘test.en-any.en’\n",
            "\n",
            "\rtest.en-any.en        0%[                    ]       0  --.-KB/s               \rtest.en-any.en      100%[===================>] 271.28K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2020-06-03 19:50:56 (14.9 MB/s) - ‘test.en-any.en’ saved [277791/277791]\n",
            "\n",
            "--2020-06-03 19:50:58--  https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-nzi.en\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 205833 (201K) [text/plain]\n",
            "Saving to: ‘test.en-nzi.en’\n",
            "\n",
            "test.en-nzi.en      100%[===================>] 201.01K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2020-06-03 19:50:59 (11.1 MB/s) - ‘test.en-nzi.en’ saved [205833/205833]\n",
            "\n",
            "--2020-06-03 19:51:03--  https://raw.githubusercontent.com/juliakreutzer/masakhane/master/jw300_utils/test/test.en-nzi.nzi\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 234052 (229K) [text/plain]\n",
            "Saving to: ‘test.en-nzi.nzi’\n",
            "\n",
            "test.en-nzi.nzi     100%[===================>] 228.57K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2020-06-03 19:51:04 (13.8 MB/s) - ‘test.en-nzi.nzi’ saved [234052/234052]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NqDG-CI28y2L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9b55f430-1583-4958-a8e3-b195f61cd22d"
      },
      "source": [
        "# Read the test data to filter from train and dev splits.\n",
        "# Store english portion in set for quick filtering checks.\n",
        "en_test_sents = set()\n",
        "filter_test_sents = \"test.en-any.en\"\n",
        "j = 0\n",
        "with open(filter_test_sents) as f:\n",
        "  for line in f:\n",
        "    en_test_sents.add(line.strip())\n",
        "    j += 1\n",
        "print('Loaded {} global test sentences to filter from the training/dev data.'.format(j))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 3571 global test sentences to filter from the training/dev data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3CNdwLBCfSIl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "72ba675b-07a8-4850-ac01-0847762d3b63"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# TMX file to dataframe\n",
        "source_file = 'jw300.' + source_language\n",
        "target_file = 'jw300.' + target_language\n",
        "\n",
        "source = []\n",
        "target = []\n",
        "skip_lines = []  # Collect the line numbers of the source portion to skip the same lines for the target portion.\n",
        "with open(source_file) as f:\n",
        "    for i, line in enumerate(f):\n",
        "        # Skip sentences that are contained in the test set.\n",
        "        if line.strip() not in en_test_sents:\n",
        "            source.append(line.strip())\n",
        "        else:\n",
        "            skip_lines.append(i)             \n",
        "with open(target_file) as f:\n",
        "    for j, line in enumerate(f):\n",
        "        # Only add to corpus if corresponding source was not skipped.\n",
        "        if j not in skip_lines:\n",
        "            target.append(line.strip())\n",
        "    \n",
        "print('Loaded data and skipped {}/{} lines since contained in test set.'.format(len(skip_lines), i))\n",
        "    \n",
        "df = pd.DataFrame(zip(source, target), columns=['source_sentence', 'target_sentence'])\n",
        "# if you get TypeError: data argument can't be an iterator is because of your zip version run this below\n",
        "#df = pd.DataFrame(list(zip(source, target)), columns=['source_sentence', 'target_sentence'])\n",
        "df.head(3)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded data and skipped 5007/103908 lines since contained in test set.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source_sentence</th>\n",
              "      <th>target_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>December 1 , 2010</td>\n",
              "      <td>January 1 , 2011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Who Inhabit the Spirit Realm ?</td>\n",
              "      <td>Nwane A Wɔ Sunsum Nu Ɛleka Ne A ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>FROM OUR COVER</td>\n",
              "      <td>EDWƐKƐ MƆƆ LUMUA</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  source_sentence                    target_sentence\n",
              "0               December 1 , 2010                   January 1 , 2011\n",
              "1  Who Inhabit the Spirit Realm ?  Nwane A Wɔ Sunsum Nu Ɛleka Ne A ?\n",
              "2                  FROM OUR COVER                   EDWƐKƐ MƆƆ LUMUA"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YkuK3B4p2AkN"
      },
      "source": [
        "## Pre-processing and export\n",
        "\n",
        "It is generally a good idea to remove duplicate translations and conflicting translations from the corpus. In practice, these public corpora include some number of these that need to be cleaned.\n",
        "\n",
        "In addition we will split our data into dev/test/train and export to the filesystem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "M_2ouEOH1_1q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "6f8a6a26-2e99-4fcb-dd12-9e8c6c94eae5"
      },
      "source": [
        "# drop duplicate translations\n",
        "df_pp = df.drop_duplicates()\n",
        "\n",
        "# drop conflicting translations\n",
        "# (this is optional and something that you might want to comment out \n",
        "# depending on the size of your corpus)\n",
        "df_pp.drop_duplicates(subset='source_sentence', inplace=True)\n",
        "df_pp.drop_duplicates(subset='target_sentence', inplace=True)\n",
        "\n",
        "# Shuffle the data to remove bias in dev set selection.\n",
        "df_pp = df_pp.sample(frac=1, random_state=seed).reset_index(drop=True)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  import sys\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z_1BwAApEtMk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "8c2bc827-92a7-431e-b269-f2be71b38048"
      },
      "source": [
        "# Install fuzzy wuzzy to remove \"almost duplicate\" sentences in the\n",
        "# test and training sets.\n",
        "! pip install fuzzywuzzy\n",
        "! pip install python-Levenshtein\n",
        "import time\n",
        "from fuzzywuzzy import process\n",
        "import numpy as np\n",
        "from os import cpu_count\n",
        "from functools import partial\n",
        "from multiprocessing import Pool\n",
        "\n",
        "\n",
        "# reset the index of the training set after previous filtering\n",
        "df_pp.reset_index(drop=False, inplace=True)\n",
        "\n",
        "# Remove samples from the training data set if they \"almost overlap\" with the\n",
        "# samples in the test set.\n",
        "\n",
        "# Filtering function. Adjust pad to narrow down the candidate matches to\n",
        "# within a certain length of characters of the given sample.\n",
        "def fuzzfilter(sample, candidates, pad):\n",
        "  candidates = [x for x in candidates if len(x) <= len(sample)+pad and len(x) >= len(sample)-pad] \n",
        "  if len(candidates) > 0:\n",
        "    return process.extractOne(sample, candidates)[1]\n",
        "  else:\n",
        "    return np.nan"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fuzzywuzzy\n",
            "  Downloading https://files.pythonhosted.org/packages/43/ff/74f23998ad2f93b945c0309f825be92e04e0348e062026998b5eefef4c33/fuzzywuzzy-0.18.0-py2.py3-none-any.whl\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n",
            "Collecting python-Levenshtein\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/a9/d1785c85ebf9b7dfacd08938dd028209c34a0ea3b1bcdb895208bd40a67d/python-Levenshtein-0.12.0.tar.gz (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 3.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from python-Levenshtein) (47.1.1)\n",
            "Building wheels for collected packages: python-Levenshtein\n",
            "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.0-cp36-cp36m-linux_x86_64.whl size=144795 sha256=b0e3929b9227a2ff336925c1dd3cd7ddaa1abe1fe7b4652a154597608353ec64\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/c2/93/660fd5f7559049268ad2dc6d81c4e39e9e36518766eaf7e342\n",
            "Successfully built python-Levenshtein\n",
            "Installing collected packages: python-Levenshtein\n",
            "Successfully installed python-Levenshtein-0.12.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlnvPA9htphZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "7029f7a6-d164-4020-e450-140a6939beea"
      },
      "source": [
        "start_time = time.time()\n",
        "### iterating over pandas dataframe rows is not recomended, let use multi processing to apply the function\n",
        "\n",
        "with Pool(cpu_count()-1) as pool:\n",
        "    scores = pool.map(partial(fuzzfilter, candidates=list(en_test_sents), pad=5), df_pp['source_sentence'])\n",
        "hours, rem = divmod(time.time() - start_time, 3600)\n",
        "minutes, seconds = divmod(rem, 60)\n",
        "print(\"done in {}h:{}min:{}seconds\".format(hours, minutes, seconds))\n",
        "\n",
        "# Filter out \"almost overlapping samples\"\n",
        "df_pp = df_pp.assign(scores=scores)\n",
        "df_pp = df_pp[df_pp['scores'] < 95]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '*']\n",
            "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "done in 0.0h:28.0min:42.572519063949585seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hxxBOCA-xXhy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "outputId": "e2e29d7f-4c11-47f5-af4b-b73188a06db4"
      },
      "source": [
        "# This section does the split between train/dev for the parallel corpora then saves them as separate files\n",
        "# We use 1000 dev test and the given test set.\n",
        "import csv\n",
        "\n",
        "# Do the split between dev/train and create parallel corpora\n",
        "num_dev_patterns = 1000\n",
        "\n",
        "# Optional: lower case the corpora - this will make it easier to generalize, but without proper casing.\n",
        "if lc:  # Julia: making lowercasing optional\n",
        "    df_pp[\"source_sentence\"] = df_pp[\"source_sentence\"].str.lower()\n",
        "    df_pp[\"target_sentence\"] = df_pp[\"target_sentence\"].str.lower()\n",
        "\n",
        "# Julia: test sets are already generated\n",
        "dev = df_pp.tail(num_dev_patterns) # Herman: Error in original\n",
        "stripped = df_pp.drop(df_pp.tail(num_dev_patterns).index)\n",
        "\n",
        "with open(\"train.\"+source_language, \"w\") as src_file, open(\"train.\"+target_language, \"w\") as trg_file:\n",
        "  for index, row in stripped.iterrows():\n",
        "    src_file.write(row[\"source_sentence\"]+\"\\n\")\n",
        "    trg_file.write(row[\"target_sentence\"]+\"\\n\")\n",
        "    \n",
        "with open(\"dev.\"+source_language, \"w\") as src_file, open(\"dev.\"+target_language, \"w\") as trg_file:\n",
        "  for index, row in dev.iterrows():\n",
        "    src_file.write(row[\"source_sentence\"]+\"\\n\")\n",
        "    trg_file.write(row[\"target_sentence\"]+\"\\n\")\n",
        "\n",
        "#stripped[[\"source_sentence\"]].to_csv(\"train.\"+source_language, header=False, index=False)  # Herman: Added `header=False` everywhere\n",
        "#stripped[[\"target_sentence\"]].to_csv(\"train.\"+target_language, header=False, index=False)  # Julia: Problematic handling of quotation marks.\n",
        "\n",
        "#dev[[\"source_sentence\"]].to_csv(\"dev.\"+source_language, header=False, index=False)\n",
        "#dev[[\"target_sentence\"]].to_csv(\"dev.\"+target_language, header=False, index=False)\n",
        "\n",
        "# Doublecheck the format below. There should be no extra quotation marks or weird characters.\n",
        "! head train.*\n",
        "! head dev.*"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> train.en <==\n",
            "They left their wives and their offspring behind to die in the floodwaters , along with that society of humans .\n",
            "Yet no matter what happens , you can find comfort in the example of Job .\n",
            "Over the course of his life , however , David proved repentant when he sinned .\n",
            "Paul expressed his unwavering confidence , saying : “ In all these things we are coming off completely victorious through him that loved us . ”\n",
            "As told by Geoffrey Jackson\n",
            "Such ones were given to “ unrighteousness , wickedness , covetousness , badness , being full of envy , murder , strife , deceit , ” and other hurtful things .\n",
            "Bible prophecy , combined with current events , identifies our time as “ the last days . ”\n",
            "What tendency do young people need to resist , and why ?\n",
            "What did Cellarius find in his examination of the Bible ?\n",
            "Likely , swallows that nested in the temple area each year found it a place of safety , where they could rear their young undisturbed .\n",
            "\n",
            "==> train.nzi <==\n",
            "Bɛgyakyile bɛ ye mɔ nee bɛ mra ɛkɛ bɛhɔle na bɛ nee menli mɔɔ ɛha la kɔsɔɔti wule .\n",
            "Noko ɔnva nwo mɔɔ ɔbazi biala la , Dwobu neazo ne bahola akyekye ɛ rɛle .\n",
            "21 : 1 , 7 ) Noko akee , ɔlale anlubɛnwo ali wɔ ye ɛbɛlabɔlɛ nu wɔ mekɛ mɔɔ ɔvonle la .\n",
            "28 : 7 ) Pɔɔlo noko hanle anwondozo mɔɔ ɛnee ɔlɛ la anwo edwɛkɛ kɛ : “ Wɔ ninyɛne ɛhye mɔ kɔsɔɔti anu , yɛlua mɔɔ hulole yɛ la anwo zo yɛdi konim ! ”\n",
            "Geoffrey Jackson Anloa Edwɛkɛ\n",
            "Menli ɛhye mɔ yɛ “ ninyɛne mɔɔ le kɛ amumuyɛ , asiayɛ , anyebolo , ngunzi , nungule , kɔdiawu , konle , adalɛ , ” nee ninyɛndane gyɛne .\n",
            "Baebolo ngapezo nee ninyɛne mɔɔ ɛlɛsisi kenlensa ye la maa ɔda ali kɛ yɛ mekɛ ye le “ mekɛ mɔɔ li awieleɛ . ”\n",
            "Subane boni a ɔwɔ kɛ ngakula koati a , na kɛmɔti ɛ ?\n",
            "Duzu a Cellarius nwunle ye wɔ Baebolo ne mɔɔ ɔnleɛnleanle nu la anu a ?\n",
            "Ɔbayɛ kɛ nyamenleakatiba mɔɔ yɛle bɛ azua wɔ ɛzonlenlɛ sua ne anwo ɛvolɛ biala la nwunle kɛ ɔle ɛleka mɔɔ banebɔlɛ wɔ , mɔɔ bɛbahola bɛanlea bɛ mra ngyikyi ne mɔ wɔ ɛkɛ a .\n",
            "==> dev.en <==\n",
            "What motivated Moses to live as he did ?\n",
            "Writing Committee\n",
            "He was simply expressing a realistic view of how life in this imperfect world turns out .\n",
            "From the start , Jacob was in love with his beautiful Rachel .\n",
            "Worldwide , you vary greatly in personality , ability , experience , interests , and faith .\n",
            "Paul identified the reason : They have a conscience . ​ — Read Romans 2 : 14 , 15 .\n",
            "My wife said that she was thinking about the same thing . ”\n",
            "WHEN Anita became one of Jehovah’s Witnesses , her husband was extremely upset .\n",
            "Are you willing to accept this challenge , even giving up some of what is commonly viewed as free time ?\n",
            "God’s people needed to be reminded that they should return to Jehovah and stop putting their personal pursuits first .\n",
            "\n",
            "==> dev.nzi <==\n",
            "Duzu ati a Mosisi kpale kɛ ɔbabɔ ye ɛbɛla kɛmɔ yɛha nwolɛ edwɛkɛ la ɛ ?\n",
            "Mbulukuhɛlɛlɛ Kɔmatii\n",
            "Mɔɔ kɔ zo wɔ ewiade ɛtane ɛhye anu la anwo edwɛkɛ yɛɛ ɛnee ɔlɛka a .\n",
            "Mɔlebɛbo ne , ɛnee Gyekɔbo kulo Relahyɛle mɔɔ anwo yɛ ye fɛ la .\n",
            "Asolo bɛ subane , mɔɔ bɛkola bɛyɛ , bɛ anwubielɛ , mɔɔ bɛ nye die nwo , nee bɛ diedi wɔ ewiade amuala .\n",
            "Pɔɔlo hilele kɛ , bɛ adwenle ne ati ɔ . — Bɛgenga Wulomuma 2 : 14 , 15 .\n",
            "Me ye hanle kɛ ɛnee ɔlɛdwenle deɛ ko ne ala anwo . ”\n",
            "ANITA rayɛle Gyihova Dasevolɛ la , ɔ hu vale ɛya kpole .\n",
            "Asoo ɛlɛ ɛhulolɛ kɛ ɛ nee menli bazukoa , saa bɔbɔ ɔwɔ kɛ ɛfa wɔ mekɛ ɛbɔ afɔle a ?\n",
            "Ɛnee ɔhyia kɛ bɛkakye Nyamenle menli ne kɛ bɛzia bɛhɔ Gyihova ɛkɛ na bɛgyakyi bɛ nwonane afoa nu ninyɛne mɔɔ bɛli ɔ nzi la .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "epeCydmCyS8X"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Installation of JoeyNMT\n",
        "\n",
        "JoeyNMT is a simple, minimalist NMT package which is useful for learning and teaching. Check out the documentation for JoeyNMT [here](https://joeynmt.readthedocs.io)  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iBRMm4kMxZ8L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6d638dba-e89e-490d-e89c-dad1fcc2c67b"
      },
      "source": [
        "# Install JoeyNMT\n",
        "! git clone https://github.com/joeynmt/joeynmt.git\n",
        "! cd joeynmt; pip3 install ."
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'joeynmt'...\n",
            "remote: Enumerating objects: 2423, done.\u001b[K\n",
            "remote: Total 2423 (delta 0), reused 0 (delta 0), pack-reused 2423\u001b[K\n",
            "Receiving objects: 100% (2423/2423), 2.63 MiB | 4.06 MiB/s, done.\n",
            "Resolving deltas: 100% (1695/1695), done.\n",
            "Processing /content/joeynmt\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (0.16.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (7.0.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (1.18.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (47.1.1)\n",
            "Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (1.5.0+cu101)\n",
            "Requirement already satisfied: tensorflow>=1.14 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (2.2.0)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (0.3.1)\n",
            "Collecting sacrebleu>=1.3.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/4b/6c7a0b26a48d88f56573d11aa5058808fe0d36ba40951287894f943556b5/sacrebleu-1.4.10-py3-none-any.whl (60kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 4.3MB/s \n",
            "\u001b[?25hCollecting subword-nmt\n",
            "  Downloading https://files.pythonhosted.org/packages/74/60/6600a7bc09e7ab38bc53a48a20d8cae49b837f93f5842a41fe513a694912/subword_nmt-0.3.7-py2.py3-none-any.whl\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (3.2.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (0.10.1)\n",
            "Collecting pyyaml>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 14.4MB/s \n",
            "\u001b[?25hCollecting pylint\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/6e/36419ec1bd2208e157dff7fc3e565b185394c0dc4901e9e2f983cb1d4b7f/pylint-2.5.2-py3-none-any.whl (324kB)\n",
            "\u001b[K     |████████████████████████████████| 327kB 18.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: six==1.12 in /usr/local/lib/python3.6/dist-packages (from joeynmt==0.0.1) (1.12.0)\n",
            "Collecting wrapt==1.11.1\n",
            "  Downloading https://files.pythonhosted.org/packages/67/b2/0f71ca90b0ade7fad27e3d20327c996c6252a2ffe88f50a95bba7434eda9/wrapt-1.11.1.tar.gz\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (3.2.1)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (2.10.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.3.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.2.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.6.3)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (3.10.0)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.4.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (2.2.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.29.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (1.1.0)\n",
            "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (2.2.2)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.34.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.14->joeynmt==0.0.1) (0.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext->joeynmt==0.0.1) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext->joeynmt==0.0.1) (2.23.0)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/53/84/7b3146ec6378d28abc73ab484f09f47dfa008ad6f03f33d90a369f880e25/portalocker-1.7.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt==0.0.1) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt==0.0.1) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt==0.0.1) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->joeynmt==0.0.1) (1.2.0)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.6/dist-packages (from seaborn->joeynmt==0.0.1) (1.0.4)\n",
            "Collecting toml>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/9f/e1/1b40b80f2e1663a6b9f497123c11d7d988c0919abbf3c3f2688e448c5363/toml-0.10.1-py2.py3-none-any.whl\n",
            "Collecting isort<5,>=4.2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/b0/c121fd1fa3419ea9bfd55c7f9c4fedfec5143208d8c7ad3ce3db6c623c21/isort-4.3.21-py2.py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.8MB/s \n",
            "\u001b[?25hCollecting astroid<=2.5,>=2.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/c9/e9c2642dfb169590fb8bdb395f9329da042ee559c2ae7c1e612a3e5f40b4/astroid-2.4.1-py3-none-any.whl (214kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 19.8MB/s \n",
            "\u001b[?25hCollecting mccabe<0.7,>=0.6\n",
            "  Downloading https://files.pythonhosted.org/packages/87/89/479dc97e18549e21354893e4ee4ef36db1d237534982482c3681ee6e7b57/mccabe-0.6.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=1.14->joeynmt==0.0.1) (0.4.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=1.14->joeynmt==0.0.1) (1.7.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=1.14->joeynmt==0.0.1) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=1.14->joeynmt==0.0.1) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow>=1.14->joeynmt==0.0.1) (1.6.0.post3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->joeynmt==0.0.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->joeynmt==0.0.1) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->joeynmt==0.0.1) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->joeynmt==0.0.1) (3.0.4)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.22.0->seaborn->joeynmt==0.0.1) (2018.9)\n",
            "Collecting lazy-object-proxy==1.4.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0b/dd/b1e3407e9e6913cf178e506cd0dee818e58694d9a5cd1984e3f6a8b9a10f/lazy_object_proxy-1.4.3-cp36-cp36m-manylinux1_x86_64.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.9MB/s \n",
            "\u001b[?25hCollecting typed-ast<1.5,>=1.4.0; implementation_name == \"cpython\" and python_version < \"3.8\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/ed/5459080d95eb87a02fe860d447197be63b6e2b5e9ff73c2b0a85622994f4/typed_ast-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (737kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 71kB/s \n",
            "\u001b[?25hRequirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow>=1.14->joeynmt==0.0.1) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow>=1.14->joeynmt==0.0.1) (0.2.8)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow>=1.14->joeynmt==0.0.1) (3.1.1)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow>=1.14->joeynmt==0.0.1) (4.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow>=1.14->joeynmt==0.0.1) (1.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow>=1.14->joeynmt==0.0.1) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow>=1.14->joeynmt==0.0.1) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow>=1.14->joeynmt==0.0.1) (3.1.0)\n",
            "Building wheels for collected packages: joeynmt, pyyaml, wrapt\n",
            "  Building wheel for joeynmt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for joeynmt: filename=joeynmt-0.0.1-cp36-none-any.whl size=77165 sha256=ab9ecd2b7570070552ceda02578233548ad1f81eedf32b80ab8874f8482a6424\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ybpf84hl/wheels/db/01/db/751cc9f3e7f6faec127c43644ba250a3ea7ad200594aeda70a\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44621 sha256=7b30c220dd96f412c81300bba954147375bcb30106bc331a5dd08a3c9c7088b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.11.1-cp36-cp36m-linux_x86_64.whl size=67413 sha256=ac63c00d2d3493325c3c40e435cfed458a5ae60821fa7a0260ac9ce9e71991ad\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/67/41/63cbf0f6ac0a6156588b9587be4db5565f8c6d8ccef98202fc\n",
            "Successfully built joeynmt pyyaml wrapt\n",
            "Installing collected packages: portalocker, sacrebleu, subword-nmt, pyyaml, toml, isort, lazy-object-proxy, typed-ast, wrapt, astroid, mccabe, pylint, joeynmt\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Found existing installation: wrapt 1.12.1\n",
            "    Uninstalling wrapt-1.12.1:\n",
            "      Successfully uninstalled wrapt-1.12.1\n",
            "Successfully installed astroid-2.4.1 isort-4.3.21 joeynmt-0.0.1 lazy-object-proxy-1.4.3 mccabe-0.6.1 portalocker-1.7.0 pylint-2.5.2 pyyaml-5.3.1 sacrebleu-1.4.10 subword-nmt-0.3.7 toml-0.10.1 typed-ast-1.4.1 wrapt-1.11.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AaE77Tcppex9"
      },
      "source": [
        "# Preprocessing the Data into Subword BPE Tokens\n",
        "\n",
        "- One of the most powerful improvements for agglutinative languages (a feature of most Bantu languages) is using BPE tokenization [ (Sennrich, 2015) ](https://arxiv.org/abs/1508.07909).\n",
        "\n",
        "- It was also shown that by optimizing the umber of BPE codes we significantly improve results for low-resourced languages [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021) [(Martinus, 2019)](https://arxiv.org/abs/1906.05685)\n",
        "\n",
        "- Below we have the scripts for doing BPE tokenization of our data. We use 4000 tokens as recommended by [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021). You do not need to change anything. Simply running the below will be suitable. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H-TyjtmXB1mL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "2fc3145e-eb5f-4bd7-eb2c-a3e466739f6f"
      },
      "source": [
        "# One of the huge boosts in NMT performance was to use a different method of tokenizing. \n",
        "# Usually, NMT would tokenize by words. However, using a method called BPE gave amazing boosts to performance\n",
        "\n",
        "# Do subword NMT\n",
        "from os import path\n",
        "os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\n",
        "os.environ[\"tgt\"] = target_language\n",
        "\n",
        "# Learn BPEs on the training data.\n",
        "os.environ[\"data_path\"] = path.join(\"joeynmt\", \"data\", source_language + target_language) # Herman! \n",
        "! subword-nmt learn-joint-bpe-and-vocab --input train.$src train.$tgt -s 4000 -o bpe.codes.4000 --write-vocabulary vocab.$src vocab.$tgt\n",
        "\n",
        "# Apply BPE splits to the development and test data.\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < train.$src > train.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < train.$tgt > train.bpe.$tgt\n",
        "\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < dev.$src > dev.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < dev.$tgt > dev.bpe.$tgt\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < test.$src > test.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < test.$tgt > test.bpe.$tgt\n",
        "\n",
        "# Create directory, move everyone we care about to the correct location\n",
        "! mkdir -p $data_path\n",
        "! cp train.* $data_path\n",
        "! cp test.* $data_path\n",
        "! cp dev.* $data_path\n",
        "! cp bpe.codes.4000 $data_path\n",
        "! ls $data_path\n",
        "\n",
        "# Also move everything we care about to a mounted location in google drive (relevant if running in colab) at gdrive_path\n",
        "! cp train.* \"$gdrive_path\"\n",
        "! cp test.* \"$gdrive_path\"\n",
        "! cp dev.* \"$gdrive_path\"\n",
        "! cp bpe.codes.4000 \"$gdrive_path\"\n",
        "! ls \"$gdrive_path\"\n",
        "\n",
        "# Create that vocab using build_vocab\n",
        "! sudo chmod 777 joeynmt/scripts/build_vocab.py\n",
        "! joeynmt/scripts/build_vocab.py joeynmt/data/$src$tgt/train.bpe.$src joeynmt/data/$src$tgt/train.bpe.$tgt --output_path joeynmt/data/$src$tgt/vocab.txt\n",
        "\n",
        "# Some output\n",
        "! echo \"BPE Xhosa Sentences\"\n",
        "! tail -n 5 test.bpe.$tgt\n",
        "! echo \"Combined BPE Vocab\"\n",
        "! tail -n 10 joeynmt/data/$src$tgt/vocab.txt  # Herman"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bpe.codes.4000\tdev.en\t     test.bpe.nzi    test.nzi\t    train.en\n",
            "dev.bpe.en\tdev.nzi      test.en\t     train.bpe.en   train.nzi\n",
            "dev.bpe.nzi\ttest.bpe.en  test.en-any.en  train.bpe.nzi\n",
            "bpe.codes.4000\tdev.en\t     test.bpe.nzi    test.nzi\t    train.en\n",
            "dev.bpe.en\tdev.nzi      test.en\t     train.bpe.en   train.nzi\n",
            "dev.bpe.nzi\ttest.bpe.en  test.en-any.en  train.bpe.nzi\n",
            "BPE Xhosa Sentences\n",
            "Edwɛndolɛ 6@@ 4 : 1 - 4 maanle menwunle kɛ ɔnle kɛ meyɛ sonla mɔɔ awie mɔ sɛlɛ Gyihova kɛ ɔbɔ bɛ nwo bane ɔvi me nwo a !\n",
            "Eza menwunle kɛ saa me@@ ang@@ y@@ akyi nd@@ ɛne@@ yelɛ a ɔnrɛ@@ maa men@@ rɛ@@ yɛ neazo kpalɛ yɛɛ ɔba@@ gua Gyihova duma ne anwo ev@@ inli . ”\n",
            "L@@ in@@ da : “ Men@@ zukoale yɛ tr@@ ate ne mɔ anu edwɛkɛ ne amaa me@@ a@@ hola me@@ ava me@@ amaa .\n",
            "Menli mɔɔ bɛfa bɛ nwo bɛ@@ wula daselɛlilɛ ne afoa ngakyile nu mɔɔ me nee bɛ tu la ɛboa me kpalɛ .\n",
            "Eza me@@ tɛ@@ fa me nwo me@@ tɛ@@ to Gyihova anwo zo wɔ asɔneyɛlɛ nu . ”\n",
            "Combined BPE Vocab\n",
            "­\n",
            "yelelɛ\n",
            "Z\n",
            "effor@@\n",
            "Ç@@\n",
            "laese\n",
            "inyɛne\n",
            "agɔ@@\n",
            "Kelese@@\n",
            "soo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IlMitUHR8Qy-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "3bfdc4fe-ae69-44ab-acd6-78ce381eac1a"
      },
      "source": [
        "# Also move everything we care about to a mounted location in google drive (relevant if running in colab) at gdrive_path\n",
        "! cp train.* \"$gdrive_path\"\n",
        "! cp test.* \"$gdrive_path\"\n",
        "! cp dev.* \"$gdrive_path\"\n",
        "! cp bpe.codes.4000 \"$gdrive_path\"\n",
        "! ls \"$gdrive_path\""
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bpe.codes.4000\tdev.en\t     test.bpe.nzi    test.nzi\t    train.en\n",
            "dev.bpe.en\tdev.nzi      test.en\t     train.bpe.en   train.nzi\n",
            "dev.bpe.nzi\ttest.bpe.en  test.en-any.en  train.bpe.nzi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ixmzi60WsUZ8"
      },
      "source": [
        "# Creating the JoeyNMT Config\n",
        "\n",
        "JoeyNMT requires a yaml config. We provide a template below. We've also set a number of defaults with it, that you may play with!\n",
        "\n",
        "- We used Transformer architecture \n",
        "- We set our dropout to reasonably high: 0.3 (recommended in  [(Sennrich, 2019)](https://www.aclweb.org/anthology/P19-1021))\n",
        "\n",
        "Things worth playing with:\n",
        "- The batch size (also recommended to change for low-resourced languages)\n",
        "- The number of epochs (we've set it at 30 just so it runs in about an hour, for testing purposes)\n",
        "- The decoder options (beam_size, alpha)\n",
        "- Evaluation metrics (BLEU versus Crhf4)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PIs1lY2hxMsl",
        "colab": {}
      },
      "source": [
        "# This creates the config file for our JoeyNMT system. It might seem overwhelming so we've provided a couple of useful parameters you'll need to update\n",
        "# (You can of course play with all the parameters if you'd like!)\n",
        "\n",
        "name = '%s%s' % (source_language, target_language)\n",
        "gdrive_path = os.environ[\"gdrive_path\"]\n",
        "\n",
        "# Create the config\n",
        "config = \"\"\"\n",
        "name: \"{name}_transformer\"\n",
        "\n",
        "data:\n",
        "    src: \"{source_language}\"\n",
        "    trg: \"{target_language}\"\n",
        "    train: \"data/{name}/train.bpe\"\n",
        "    dev:   \"data/{name}/dev.bpe\"\n",
        "    test:  \"data/{name}/test.bpe\"\n",
        "    level: \"bpe\"\n",
        "    lowercase: False\n",
        "    max_sent_length: 100\n",
        "    src_vocab: \"data/{name}/vocab.txt\"\n",
        "    trg_vocab: \"data/{name}/vocab.txt\"\n",
        "\n",
        "testing:\n",
        "    beam_size: 5\n",
        "    alpha: 1.0\n",
        "\n",
        "training:\n",
        "    #load_model: \"{gdrive_path}/models/{name}_transformer/1.ckpt\" # if uncommented, load a pre-trained model from this checkpoint\n",
        "    random_seed: 42\n",
        "    optimizer: \"adam\"\n",
        "    normalization: \"tokens\"\n",
        "    adam_betas: [0.9, 0.999] \n",
        "    scheduling: \"plateau\"           # TODO: try switching from plateau to Noam scheduling\n",
        "    patience: 5                     # For plateau: decrease learning rate by decrease_factor if validation score has not improved for this many validation rounds.\n",
        "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
        "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
        "    decrease_factor: 0.7\n",
        "    loss: \"crossentropy\"\n",
        "    learning_rate: 0.0003\n",
        "    learning_rate_min: 0.00000001\n",
        "    weight_decay: 0.0\n",
        "    label_smoothing: 0.1\n",
        "    batch_size: 4096\n",
        "    batch_type: \"token\"\n",
        "    eval_batch_size: 3600\n",
        "    eval_batch_type: \"token\"\n",
        "    batch_multiplier: 1\n",
        "    early_stopping_metric: \"ppl\"\n",
        "    epochs: 30                     # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
        "    validation_freq: 1000          # TODO: Set to at least once per epoch.\n",
        "    logging_freq: 100\n",
        "    eval_metric: \"bleu\"\n",
        "    model_dir: \"models/{name}_transformer\"\n",
        "    overwrite: False               # TODO: Set to True if you want to overwrite possibly existing models. \n",
        "    shuffle: True\n",
        "    use_cuda: True\n",
        "    max_output_length: 100\n",
        "    print_valid_sents: [0, 1, 2, 3]\n",
        "    keep_last_ckpts: 3\n",
        "\n",
        "model:\n",
        "    initializer: \"xavier\"\n",
        "    bias_initializer: \"zeros\"\n",
        "    init_gain: 1.0\n",
        "    embed_initializer: \"xavier\"\n",
        "    embed_init_gain: 1.0\n",
        "    tied_embeddings: True\n",
        "    tied_softmax: True\n",
        "    encoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4             # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256   # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "    decoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 4              # TODO: Increase to 8 for larger data.\n",
        "        embeddings:\n",
        "            embedding_dim: 256    # TODO: Increase to 512 for larger data.\n",
        "            scale: True\n",
        "            dropout: 0.2\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 256         # TODO: Increase to 512 for larger data.\n",
        "        ff_size: 1024            # TODO: Increase to 2048 for larger data.\n",
        "        dropout: 0.3\n",
        "\"\"\".format(name=name, gdrive_path=os.environ[\"gdrive_path\"], source_language=source_language, target_language=target_language)\n",
        "with open(\"joeynmt/configs/transformer_{name}.yaml\".format(name=name),'w') as f:\n",
        "    f.write(config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pIifxE3Qzuvs"
      },
      "source": [
        "# Train the Model\n",
        "\n",
        "This single line of joeynmt runs the training using the config we made above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6ZBPFwT94WpI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "51ef9660-43f4-46c3-8e90-5e3c79935c2a"
      },
      "source": [
        "# Train the model\n",
        "# You can press Ctrl-C to stop. And then run the next cell to save your checkpoints! \n",
        "!cd joeynmt; python3 -m joeynmt train configs/transformer_$src$tgt.yaml"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-03 20:23:26,823 Hello! This is Joey-NMT.\n",
            "2020-06-03 20:23:26.938710: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-06-03 20:23:28,227 Total params: 12129792\n",
            "2020-06-03 20:23:28,228 Trainable parameters: ['decoder.layer_norm.bias', 'decoder.layer_norm.weight', 'decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'decoder.layers.1.dec_layer_norm.bias', 'decoder.layers.1.dec_layer_norm.weight', 'decoder.layers.1.feed_forward.layer_norm.bias', 'decoder.layers.1.feed_forward.layer_norm.weight', 'decoder.layers.1.feed_forward.pwff_layer.0.bias', 'decoder.layers.1.feed_forward.pwff_layer.0.weight', 'decoder.layers.1.feed_forward.pwff_layer.3.bias', 'decoder.layers.1.feed_forward.pwff_layer.3.weight', 'decoder.layers.1.src_trg_att.k_layer.bias', 'decoder.layers.1.src_trg_att.k_layer.weight', 'decoder.layers.1.src_trg_att.output_layer.bias', 'decoder.layers.1.src_trg_att.output_layer.weight', 'decoder.layers.1.src_trg_att.q_layer.bias', 'decoder.layers.1.src_trg_att.q_layer.weight', 'decoder.layers.1.src_trg_att.v_layer.bias', 'decoder.layers.1.src_trg_att.v_layer.weight', 'decoder.layers.1.trg_trg_att.k_layer.bias', 'decoder.layers.1.trg_trg_att.k_layer.weight', 'decoder.layers.1.trg_trg_att.output_layer.bias', 'decoder.layers.1.trg_trg_att.output_layer.weight', 'decoder.layers.1.trg_trg_att.q_layer.bias', 'decoder.layers.1.trg_trg_att.q_layer.weight', 'decoder.layers.1.trg_trg_att.v_layer.bias', 'decoder.layers.1.trg_trg_att.v_layer.weight', 'decoder.layers.1.x_layer_norm.bias', 'decoder.layers.1.x_layer_norm.weight', 'decoder.layers.2.dec_layer_norm.bias', 'decoder.layers.2.dec_layer_norm.weight', 'decoder.layers.2.feed_forward.layer_norm.bias', 'decoder.layers.2.feed_forward.layer_norm.weight', 'decoder.layers.2.feed_forward.pwff_layer.0.bias', 'decoder.layers.2.feed_forward.pwff_layer.0.weight', 'decoder.layers.2.feed_forward.pwff_layer.3.bias', 'decoder.layers.2.feed_forward.pwff_layer.3.weight', 'decoder.layers.2.src_trg_att.k_layer.bias', 'decoder.layers.2.src_trg_att.k_layer.weight', 'decoder.layers.2.src_trg_att.output_layer.bias', 'decoder.layers.2.src_trg_att.output_layer.weight', 'decoder.layers.2.src_trg_att.q_layer.bias', 'decoder.layers.2.src_trg_att.q_layer.weight', 'decoder.layers.2.src_trg_att.v_layer.bias', 'decoder.layers.2.src_trg_att.v_layer.weight', 'decoder.layers.2.trg_trg_att.k_layer.bias', 'decoder.layers.2.trg_trg_att.k_layer.weight', 'decoder.layers.2.trg_trg_att.output_layer.bias', 'decoder.layers.2.trg_trg_att.output_layer.weight', 'decoder.layers.2.trg_trg_att.q_layer.bias', 'decoder.layers.2.trg_trg_att.q_layer.weight', 'decoder.layers.2.trg_trg_att.v_layer.bias', 'decoder.layers.2.trg_trg_att.v_layer.weight', 'decoder.layers.2.x_layer_norm.bias', 'decoder.layers.2.x_layer_norm.weight', 'decoder.layers.3.dec_layer_norm.bias', 'decoder.layers.3.dec_layer_norm.weight', 'decoder.layers.3.feed_forward.layer_norm.bias', 'decoder.layers.3.feed_forward.layer_norm.weight', 'decoder.layers.3.feed_forward.pwff_layer.0.bias', 'decoder.layers.3.feed_forward.pwff_layer.0.weight', 'decoder.layers.3.feed_forward.pwff_layer.3.bias', 'decoder.layers.3.feed_forward.pwff_layer.3.weight', 'decoder.layers.3.src_trg_att.k_layer.bias', 'decoder.layers.3.src_trg_att.k_layer.weight', 'decoder.layers.3.src_trg_att.output_layer.bias', 'decoder.layers.3.src_trg_att.output_layer.weight', 'decoder.layers.3.src_trg_att.q_layer.bias', 'decoder.layers.3.src_trg_att.q_layer.weight', 'decoder.layers.3.src_trg_att.v_layer.bias', 'decoder.layers.3.src_trg_att.v_layer.weight', 'decoder.layers.3.trg_trg_att.k_layer.bias', 'decoder.layers.3.trg_trg_att.k_layer.weight', 'decoder.layers.3.trg_trg_att.output_layer.bias', 'decoder.layers.3.trg_trg_att.output_layer.weight', 'decoder.layers.3.trg_trg_att.q_layer.bias', 'decoder.layers.3.trg_trg_att.q_layer.weight', 'decoder.layers.3.trg_trg_att.v_layer.bias', 'decoder.layers.3.trg_trg_att.v_layer.weight', 'decoder.layers.3.x_layer_norm.bias', 'decoder.layers.3.x_layer_norm.weight', 'decoder.layers.4.dec_layer_norm.bias', 'decoder.layers.4.dec_layer_norm.weight', 'decoder.layers.4.feed_forward.layer_norm.bias', 'decoder.layers.4.feed_forward.layer_norm.weight', 'decoder.layers.4.feed_forward.pwff_layer.0.bias', 'decoder.layers.4.feed_forward.pwff_layer.0.weight', 'decoder.layers.4.feed_forward.pwff_layer.3.bias', 'decoder.layers.4.feed_forward.pwff_layer.3.weight', 'decoder.layers.4.src_trg_att.k_layer.bias', 'decoder.layers.4.src_trg_att.k_layer.weight', 'decoder.layers.4.src_trg_att.output_layer.bias', 'decoder.layers.4.src_trg_att.output_layer.weight', 'decoder.layers.4.src_trg_att.q_layer.bias', 'decoder.layers.4.src_trg_att.q_layer.weight', 'decoder.layers.4.src_trg_att.v_layer.bias', 'decoder.layers.4.src_trg_att.v_layer.weight', 'decoder.layers.4.trg_trg_att.k_layer.bias', 'decoder.layers.4.trg_trg_att.k_layer.weight', 'decoder.layers.4.trg_trg_att.output_layer.bias', 'decoder.layers.4.trg_trg_att.output_layer.weight', 'decoder.layers.4.trg_trg_att.q_layer.bias', 'decoder.layers.4.trg_trg_att.q_layer.weight', 'decoder.layers.4.trg_trg_att.v_layer.bias', 'decoder.layers.4.trg_trg_att.v_layer.weight', 'decoder.layers.4.x_layer_norm.bias', 'decoder.layers.4.x_layer_norm.weight', 'decoder.layers.5.dec_layer_norm.bias', 'decoder.layers.5.dec_layer_norm.weight', 'decoder.layers.5.feed_forward.layer_norm.bias', 'decoder.layers.5.feed_forward.layer_norm.weight', 'decoder.layers.5.feed_forward.pwff_layer.0.bias', 'decoder.layers.5.feed_forward.pwff_layer.0.weight', 'decoder.layers.5.feed_forward.pwff_layer.3.bias', 'decoder.layers.5.feed_forward.pwff_layer.3.weight', 'decoder.layers.5.src_trg_att.k_layer.bias', 'decoder.layers.5.src_trg_att.k_layer.weight', 'decoder.layers.5.src_trg_att.output_layer.bias', 'decoder.layers.5.src_trg_att.output_layer.weight', 'decoder.layers.5.src_trg_att.q_layer.bias', 'decoder.layers.5.src_trg_att.q_layer.weight', 'decoder.layers.5.src_trg_att.v_layer.bias', 'decoder.layers.5.src_trg_att.v_layer.weight', 'decoder.layers.5.trg_trg_att.k_layer.bias', 'decoder.layers.5.trg_trg_att.k_layer.weight', 'decoder.layers.5.trg_trg_att.output_layer.bias', 'decoder.layers.5.trg_trg_att.output_layer.weight', 'decoder.layers.5.trg_trg_att.q_layer.bias', 'decoder.layers.5.trg_trg_att.q_layer.weight', 'decoder.layers.5.trg_trg_att.v_layer.bias', 'decoder.layers.5.trg_trg_att.v_layer.weight', 'decoder.layers.5.x_layer_norm.bias', 'decoder.layers.5.x_layer_norm.weight', 'encoder.layer_norm.bias', 'encoder.layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'encoder.layers.4.feed_forward.layer_norm.bias', 'encoder.layers.4.feed_forward.layer_norm.weight', 'encoder.layers.4.feed_forward.pwff_layer.0.bias', 'encoder.layers.4.feed_forward.pwff_layer.0.weight', 'encoder.layers.4.feed_forward.pwff_layer.3.bias', 'encoder.layers.4.feed_forward.pwff_layer.3.weight', 'encoder.layers.4.layer_norm.bias', 'encoder.layers.4.layer_norm.weight', 'encoder.layers.4.src_src_att.k_layer.bias', 'encoder.layers.4.src_src_att.k_layer.weight', 'encoder.layers.4.src_src_att.output_layer.bias', 'encoder.layers.4.src_src_att.output_layer.weight', 'encoder.layers.4.src_src_att.q_layer.bias', 'encoder.layers.4.src_src_att.q_layer.weight', 'encoder.layers.4.src_src_att.v_layer.bias', 'encoder.layers.4.src_src_att.v_layer.weight', 'encoder.layers.5.feed_forward.layer_norm.bias', 'encoder.layers.5.feed_forward.layer_norm.weight', 'encoder.layers.5.feed_forward.pwff_layer.0.bias', 'encoder.layers.5.feed_forward.pwff_layer.0.weight', 'encoder.layers.5.feed_forward.pwff_layer.3.bias', 'encoder.layers.5.feed_forward.pwff_layer.3.weight', 'encoder.layers.5.layer_norm.bias', 'encoder.layers.5.layer_norm.weight', 'encoder.layers.5.src_src_att.k_layer.bias', 'encoder.layers.5.src_src_att.k_layer.weight', 'encoder.layers.5.src_src_att.output_layer.bias', 'encoder.layers.5.src_src_att.output_layer.weight', 'encoder.layers.5.src_src_att.q_layer.bias', 'encoder.layers.5.src_src_att.q_layer.weight', 'encoder.layers.5.src_src_att.v_layer.bias', 'encoder.layers.5.src_src_att.v_layer.weight', 'src_embed.lut.weight']\n",
            "2020-06-03 20:23:44,102 cfg.name                           : ennzi_transformer\n",
            "2020-06-03 20:23:44,102 cfg.data.src                       : en\n",
            "2020-06-03 20:23:44,102 cfg.data.trg                       : nzi\n",
            "2020-06-03 20:23:44,102 cfg.data.train                     : data/ennzi/train.bpe\n",
            "2020-06-03 20:23:44,102 cfg.data.dev                       : data/ennzi/dev.bpe\n",
            "2020-06-03 20:23:44,102 cfg.data.test                      : data/ennzi/test.bpe\n",
            "2020-06-03 20:23:44,102 cfg.data.level                     : bpe\n",
            "2020-06-03 20:23:44,103 cfg.data.lowercase                 : False\n",
            "2020-06-03 20:23:44,103 cfg.data.max_sent_length           : 100\n",
            "2020-06-03 20:23:44,103 cfg.data.src_vocab                 : data/ennzi/vocab.txt\n",
            "2020-06-03 20:23:44,103 cfg.data.trg_vocab                 : data/ennzi/vocab.txt\n",
            "2020-06-03 20:23:44,103 cfg.testing.beam_size              : 5\n",
            "2020-06-03 20:23:44,103 cfg.testing.alpha                  : 1.0\n",
            "2020-06-03 20:23:44,103 cfg.training.random_seed           : 42\n",
            "2020-06-03 20:23:44,103 cfg.training.optimizer             : adam\n",
            "2020-06-03 20:23:44,103 cfg.training.normalization         : tokens\n",
            "2020-06-03 20:23:44,103 cfg.training.adam_betas            : [0.9, 0.999]\n",
            "2020-06-03 20:23:44,103 cfg.training.scheduling            : plateau\n",
            "2020-06-03 20:23:44,103 cfg.training.patience              : 5\n",
            "2020-06-03 20:23:44,103 cfg.training.learning_rate_factor  : 0.5\n",
            "2020-06-03 20:23:44,103 cfg.training.learning_rate_warmup  : 1000\n",
            "2020-06-03 20:23:44,104 cfg.training.decrease_factor       : 0.7\n",
            "2020-06-03 20:23:44,104 cfg.training.loss                  : crossentropy\n",
            "2020-06-03 20:23:44,104 cfg.training.learning_rate         : 0.0003\n",
            "2020-06-03 20:23:44,104 cfg.training.learning_rate_min     : 1e-08\n",
            "2020-06-03 20:23:44,104 cfg.training.weight_decay          : 0.0\n",
            "2020-06-03 20:23:44,104 cfg.training.label_smoothing       : 0.1\n",
            "2020-06-03 20:23:44,104 cfg.training.batch_size            : 4096\n",
            "2020-06-03 20:23:44,104 cfg.training.batch_type            : token\n",
            "2020-06-03 20:23:44,104 cfg.training.eval_batch_size       : 3600\n",
            "2020-06-03 20:23:44,104 cfg.training.eval_batch_type       : token\n",
            "2020-06-03 20:23:44,104 cfg.training.batch_multiplier      : 1\n",
            "2020-06-03 20:23:44,104 cfg.training.early_stopping_metric : ppl\n",
            "2020-06-03 20:23:44,104 cfg.training.epochs                : 30\n",
            "2020-06-03 20:23:44,104 cfg.training.validation_freq       : 1000\n",
            "2020-06-03 20:23:44,104 cfg.training.logging_freq          : 100\n",
            "2020-06-03 20:23:44,104 cfg.training.eval_metric           : bleu\n",
            "2020-06-03 20:23:44,105 cfg.training.model_dir             : models/ennzi_transformer\n",
            "2020-06-03 20:23:44,105 cfg.training.overwrite             : False\n",
            "2020-06-03 20:23:44,105 cfg.training.shuffle               : True\n",
            "2020-06-03 20:23:44,105 cfg.training.use_cuda              : True\n",
            "2020-06-03 20:23:44,105 cfg.training.max_output_length     : 100\n",
            "2020-06-03 20:23:44,105 cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
            "2020-06-03 20:23:44,105 cfg.training.keep_last_ckpts       : 3\n",
            "2020-06-03 20:23:44,105 cfg.model.initializer              : xavier\n",
            "2020-06-03 20:23:44,105 cfg.model.bias_initializer         : zeros\n",
            "2020-06-03 20:23:44,105 cfg.model.init_gain                : 1.0\n",
            "2020-06-03 20:23:44,105 cfg.model.embed_initializer        : xavier\n",
            "2020-06-03 20:23:44,105 cfg.model.embed_init_gain          : 1.0\n",
            "2020-06-03 20:23:44,105 cfg.model.tied_embeddings          : True\n",
            "2020-06-03 20:23:44,105 cfg.model.tied_softmax             : True\n",
            "2020-06-03 20:23:44,105 cfg.model.encoder.type             : transformer\n",
            "2020-06-03 20:23:44,106 cfg.model.encoder.num_layers       : 6\n",
            "2020-06-03 20:23:44,106 cfg.model.encoder.num_heads        : 4\n",
            "2020-06-03 20:23:44,106 cfg.model.encoder.embeddings.embedding_dim : 256\n",
            "2020-06-03 20:23:44,106 cfg.model.encoder.embeddings.scale : True\n",
            "2020-06-03 20:23:44,106 cfg.model.encoder.embeddings.dropout : 0.2\n",
            "2020-06-03 20:23:44,106 cfg.model.encoder.hidden_size      : 256\n",
            "2020-06-03 20:23:44,106 cfg.model.encoder.ff_size          : 1024\n",
            "2020-06-03 20:23:44,106 cfg.model.encoder.dropout          : 0.3\n",
            "2020-06-03 20:23:44,106 cfg.model.decoder.type             : transformer\n",
            "2020-06-03 20:23:44,106 cfg.model.decoder.num_layers       : 6\n",
            "2020-06-03 20:23:44,106 cfg.model.decoder.num_heads        : 4\n",
            "2020-06-03 20:23:44,106 cfg.model.decoder.embeddings.embedding_dim : 256\n",
            "2020-06-03 20:23:44,106 cfg.model.decoder.embeddings.scale : True\n",
            "2020-06-03 20:23:44,106 cfg.model.decoder.embeddings.dropout : 0.2\n",
            "2020-06-03 20:23:44,106 cfg.model.decoder.hidden_size      : 256\n",
            "2020-06-03 20:23:44,106 cfg.model.decoder.ff_size          : 1024\n",
            "2020-06-03 20:23:44,107 cfg.model.decoder.dropout          : 0.3\n",
            "2020-06-03 20:23:44,107 Data set sizes: \n",
            "\ttrain 89979,\n",
            "\tvalid 1000,\n",
            "\ttest 2709\n",
            "2020-06-03 20:23:44,107 First training example:\n",
            "\t[SRC] They left their wi@@ ves and their off@@ sp@@ ring be@@ hind to die in the f@@ loo@@ dw@@ at@@ ers , along with that soci@@ ety of humans .\n",
            "\t[TRG] Bɛ@@ gyakyile bɛ ye mɔ nee bɛ mra ɛkɛ bɛhɔle na bɛ nee menli mɔɔ ɛha la kɔsɔɔti wule .\n",
            "2020-06-03 20:23:44,107 First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) a (7) the (8) ne (9) la\n",
            "2020-06-03 20:23:44,108 First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) . (5) , (6) a (7) the (8) ne (9) la\n",
            "2020-06-03 20:23:44,108 Number of Src words (types): 4178\n",
            "2020-06-03 20:23:44,108 Number of Trg words (types): 4178\n",
            "2020-06-03 20:23:44,108 Model(\n",
            "\tencoder=TransformerEncoder(num_layers=6, num_heads=4),\n",
            "\tdecoder=TransformerDecoder(num_layers=6, num_heads=4),\n",
            "\tsrc_embed=Embeddings(embedding_dim=256, vocab_size=4178),\n",
            "\ttrg_embed=Embeddings(embedding_dim=256, vocab_size=4178))\n",
            "2020-06-03 20:23:44,111 EPOCH 1\n",
            "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of nonzero is deprecated:\n",
            "\tnonzero(Tensor input, *, Tensor out)\n",
            "Consider using one of the following signatures instead:\n",
            "\tnonzero(Tensor input, *, bool as_tuple)\n",
            "2020-06-03 20:23:55,089 Epoch   1 Step:      100 Batch Loss:     5.215174 Tokens per Sec:    20103, Lr: 0.000300\n",
            "2020-06-03 20:24:05,681 Epoch   1 Step:      200 Batch Loss:     5.053903 Tokens per Sec:    20829, Lr: 0.000300\n",
            "2020-06-03 20:24:16,219 Epoch   1 Step:      300 Batch Loss:     4.751307 Tokens per Sec:    20972, Lr: 0.000300\n",
            "2020-06-03 20:24:26,847 Epoch   1 Step:      400 Batch Loss:     4.480493 Tokens per Sec:    20962, Lr: 0.000300\n",
            "2020-06-03 20:24:37,466 Epoch   1 Step:      500 Batch Loss:     4.542974 Tokens per Sec:    20795, Lr: 0.000300\n",
            "2020-06-03 20:24:48,016 Epoch   1 Step:      600 Batch Loss:     4.287178 Tokens per Sec:    20881, Lr: 0.000300\n",
            "2020-06-03 20:24:58,432 Epoch   1 Step:      700 Batch Loss:     3.500154 Tokens per Sec:    20562, Lr: 0.000300\n",
            "2020-06-03 20:25:08,886 Epoch   1 Step:      800 Batch Loss:     4.077501 Tokens per Sec:    20644, Lr: 0.000300\n",
            "2020-06-03 20:25:19,400 Epoch   1 Step:      900 Batch Loss:     3.758924 Tokens per Sec:    20294, Lr: 0.000300\n",
            "2020-06-03 20:25:28,871 Epoch   1: total training loss 4415.06\n",
            "2020-06-03 20:25:28,871 EPOCH 2\n",
            "2020-06-03 20:25:29,978 Epoch   2 Step:     1000 Batch Loss:     3.813756 Tokens per Sec:    18086, Lr: 0.000300\n",
            "2020-06-03 20:25:50,603 Hooray! New best validation result [ppl]!\n",
            "2020-06-03 20:25:50,603 Saving new checkpoint.\n",
            "2020-06-03 20:25:50,813 Example #0\n",
            "2020-06-03 20:25:50,813 \tSource:     What motivated Moses to live as he did ?\n",
            "2020-06-03 20:25:50,814 \tReference:  Duzu ati a Mosisi kpale kɛ ɔbabɔ ye ɛbɛla kɛmɔ yɛha nwolɛ edwɛkɛ la ɛ ?\n",
            "2020-06-03 20:25:50,814 \tHypothesis: Duzu a ɔwɔ kɛ Nyamenle bava ye a ?\n",
            "2020-06-03 20:25:50,814 Example #1\n",
            "2020-06-03 20:25:50,814 \tSource:     Writing Committee\n",
            "2020-06-03 20:25:50,814 \tReference:  Mbulukuhɛlɛlɛ Kɔmatii\n",
            "2020-06-03 20:25:50,814 \tHypothesis: Ɛlevolɛ Ne Ae\n",
            "2020-06-03 20:25:50,814 Example #2\n",
            "2020-06-03 20:25:50,814 \tSource:     He was simply expressing a realistic view of how life in this imperfect world turns out .\n",
            "2020-06-03 20:25:50,814 \tReference:  Mɔɔ kɔ zo wɔ ewiade ɛtane ɛhye anu la anwo edwɛkɛ yɛɛ ɛnee ɔlɛka a .\n",
            "2020-06-03 20:25:50,814 \tHypothesis: Ɛnee Gyisɛse hanle kɛ , noko menli mɔɔ bɛbɔ ye la anwo edwɛkɛ ne anu .\n",
            "2020-06-03 20:25:50,814 Example #3\n",
            "2020-06-03 20:25:50,814 \tSource:     From the start , Jacob was in love with his beautiful Rachel .\n",
            "2020-06-03 20:25:50,814 \tReference:  Mɔlebɛbo ne , ɛnee Gyekɔbo kulo Relahyɛle mɔɔ anwo yɛ ye fɛ la .\n",
            "2020-06-03 20:25:50,814 \tHypothesis: Ɛnee ɛnee ɛnee ɛnee ɛnee ɛnee ɛnee ɛnee ɛnee ɛnee ɛnee ɛnee ɛnee bɛvile ye wɔ ɛvolɛ ne anu .\n",
            "2020-06-03 20:25:50,815 Validation result (greedy) at epoch   2, step     1000: bleu:   1.48, loss: 89169.7422, ppl:  41.8821, duration: 20.8367s\n",
            "2020-06-03 20:26:01,450 Epoch   2 Step:     1100 Batch Loss:     3.588871 Tokens per Sec:    20911, Lr: 0.000300\n",
            "2020-06-03 20:26:12,068 Epoch   2 Step:     1200 Batch Loss:     4.165189 Tokens per Sec:    20433, Lr: 0.000300\n",
            "2020-06-03 20:26:22,698 Epoch   2 Step:     1300 Batch Loss:     4.046685 Tokens per Sec:    20440, Lr: 0.000300\n",
            "2020-06-03 20:26:33,147 Epoch   2 Step:     1400 Batch Loss:     3.572782 Tokens per Sec:    20506, Lr: 0.000300\n",
            "2020-06-03 20:26:43,764 Epoch   2 Step:     1500 Batch Loss:     3.415729 Tokens per Sec:    20749, Lr: 0.000300\n",
            "2020-06-03 20:26:54,252 Epoch   2 Step:     1600 Batch Loss:     2.857646 Tokens per Sec:    20742, Lr: 0.000300\n",
            "2020-06-03 20:27:04,879 Epoch   2 Step:     1700 Batch Loss:     3.816541 Tokens per Sec:    21108, Lr: 0.000300\n",
            "2020-06-03 20:27:15,347 Epoch   2 Step:     1800 Batch Loss:     3.162721 Tokens per Sec:    20819, Lr: 0.000300\n",
            "2020-06-03 20:27:25,923 Epoch   2 Step:     1900 Batch Loss:     3.430209 Tokens per Sec:    20669, Lr: 0.000300\n",
            "2020-06-03 20:27:34,525 Epoch   2: total training loss 3479.51\n",
            "2020-06-03 20:27:34,526 EPOCH 3\n",
            "2020-06-03 20:27:36,631 Epoch   3 Step:     2000 Batch Loss:     2.755164 Tokens per Sec:    18980, Lr: 0.000300\n",
            "2020-06-03 20:27:52,287 Hooray! New best validation result [ppl]!\n",
            "2020-06-03 20:27:52,287 Saving new checkpoint.\n",
            "2020-06-03 20:27:52,489 Example #0\n",
            "2020-06-03 20:27:52,489 \tSource:     What motivated Moses to live as he did ?\n",
            "2020-06-03 20:27:52,489 \tReference:  Duzu ati a Mosisi kpale kɛ ɔbabɔ ye ɛbɛla kɛmɔ yɛha nwolɛ edwɛkɛ la ɛ ?\n",
            "2020-06-03 20:27:52,489 \tHypothesis: Duzu a Devidi yɛle ye a ?\n",
            "2020-06-03 20:27:52,489 Example #1\n",
            "2020-06-03 20:27:52,489 \tSource:     Writing Committee\n",
            "2020-06-03 20:27:52,489 \tReference:  Mbulukuhɛlɛlɛ Kɔmatii\n",
            "2020-06-03 20:27:52,489 \tHypothesis: Bɛmaa Ɛwɔkɛ Amgba\n",
            "2020-06-03 20:27:52,489 Example #2\n",
            "2020-06-03 20:27:52,490 \tSource:     He was simply expressing a realistic view of how life in this imperfect world turns out .\n",
            "2020-06-03 20:27:52,490 \tReference:  Mɔɔ kɔ zo wɔ ewiade ɛtane ɛhye anu la anwo edwɛkɛ yɛɛ ɛnee ɔlɛka a .\n",
            "2020-06-03 20:27:52,490 \tHypothesis: Ɔnwunle kɛ ɔsi kpɔkɛ mɔɔ wɔ ewiade ne anu la anwo .\n",
            "2020-06-03 20:27:52,490 Example #3\n",
            "2020-06-03 20:27:52,490 \tSource:     From the start , Jacob was in love with his beautiful Rachel .\n",
            "2020-06-03 20:27:52,490 \tReference:  Mɔlebɛbo ne , ɛnee Gyekɔbo kulo Relahyɛle mɔɔ anwo yɛ ye fɛ la .\n",
            "2020-06-03 20:27:52,490 \tHypothesis: Wɔ ɛvolɛ nwiɔ ne anu , ɛnee ɛnee ɔ nye die nwo .\n",
            "2020-06-03 20:27:52,490 Validation result (greedy) at epoch   3, step     2000: bleu:   3.73, loss: 75706.9062, ppl:  23.8306, duration: 15.8587s\n",
            "2020-06-03 20:28:03,042 Epoch   3 Step:     2100 Batch Loss:     3.245998 Tokens per Sec:    20382, Lr: 0.000300\n",
            "2020-06-03 20:28:13,536 Epoch   3 Step:     2200 Batch Loss:     3.554363 Tokens per Sec:    20319, Lr: 0.000300\n",
            "2020-06-03 20:28:24,222 Epoch   3 Step:     2300 Batch Loss:     3.261512 Tokens per Sec:    20703, Lr: 0.000300\n",
            "2020-06-03 20:28:34,800 Epoch   3 Step:     2400 Batch Loss:     3.465734 Tokens per Sec:    21044, Lr: 0.000300\n",
            "2020-06-03 20:28:45,294 Epoch   3 Step:     2500 Batch Loss:     2.772790 Tokens per Sec:    20759, Lr: 0.000300\n",
            "2020-06-03 20:28:55,839 Epoch   3 Step:     2600 Batch Loss:     3.178838 Tokens per Sec:    20680, Lr: 0.000300\n",
            "2020-06-03 20:29:06,473 Epoch   3 Step:     2700 Batch Loss:     3.355007 Tokens per Sec:    21116, Lr: 0.000300\n",
            "2020-06-03 20:29:16,913 Epoch   3 Step:     2800 Batch Loss:     2.725268 Tokens per Sec:    20240, Lr: 0.000300\n",
            "2020-06-03 20:29:27,540 Epoch   3 Step:     2900 Batch Loss:     3.603246 Tokens per Sec:    21250, Lr: 0.000300\n",
            "2020-06-03 20:29:35,070 Epoch   3: total training loss 3090.35\n",
            "2020-06-03 20:29:35,071 EPOCH 4\n",
            "2020-06-03 20:29:38,112 Epoch   4 Step:     3000 Batch Loss:     2.366612 Tokens per Sec:    19804, Lr: 0.000300\n",
            "2020-06-03 20:29:49,893 Hooray! New best validation result [ppl]!\n",
            "2020-06-03 20:29:49,893 Saving new checkpoint.\n",
            "2020-06-03 20:29:50,103 Example #0\n",
            "2020-06-03 20:29:50,103 \tSource:     What motivated Moses to live as he did ?\n",
            "2020-06-03 20:29:50,103 \tReference:  Duzu ati a Mosisi kpale kɛ ɔbabɔ ye ɛbɛla kɛmɔ yɛha nwolɛ edwɛkɛ la ɛ ?\n",
            "2020-06-03 20:29:50,103 \tHypothesis: Duzu a Mosisi yɛle a ?\n",
            "2020-06-03 20:29:50,104 Example #1\n",
            "2020-06-03 20:29:50,104 \tSource:     Writing Committee\n",
            "2020-06-03 20:29:50,104 \tReference:  Mbulukuhɛlɛlɛ Kɔmatii\n",
            "2020-06-03 20:29:50,104 \tHypothesis: Bɛboa Bɛboa\n",
            "2020-06-03 20:29:50,104 Example #2\n",
            "2020-06-03 20:29:50,104 \tSource:     He was simply expressing a realistic view of how life in this imperfect world turns out .\n",
            "2020-06-03 20:29:50,104 \tReference:  Mɔɔ kɔ zo wɔ ewiade ɛtane ɛhye anu la anwo edwɛkɛ yɛɛ ɛnee ɔlɛka a .\n",
            "2020-06-03 20:29:50,104 \tHypothesis: Ɛnee ɔze kɛ ɔbaha kɛ ewiade ɛhye mɔɔ wɔ ewiade ɛhye anu la anu .\n",
            "2020-06-03 20:29:50,104 Example #3\n",
            "2020-06-03 20:29:50,105 \tSource:     From the start , Jacob was in love with his beautiful Rachel .\n",
            "2020-06-03 20:29:50,105 \tReference:  Mɔlebɛbo ne , ɛnee Gyekɔbo kulo Relahyɛle mɔɔ anwo yɛ ye fɛ la .\n",
            "2020-06-03 20:29:50,105 \tHypothesis: Ɛnee Dwosefi nee ye lɛ diedi kpole wɔ Sendulɛ ne anu .\n",
            "2020-06-03 20:29:50,105 Validation result (greedy) at epoch   4, step     3000: bleu:   5.97, loss: 67883.8359, ppl:  17.1724, duration: 11.9921s\n",
            "2020-06-03 20:30:00,645 Epoch   4 Step:     3100 Batch Loss:     3.023391 Tokens per Sec:    20802, Lr: 0.000300\n",
            "2020-06-03 20:30:11,124 Epoch   4 Step:     3200 Batch Loss:     3.195156 Tokens per Sec:    20823, Lr: 0.000300\n",
            "2020-06-03 20:30:21,646 Epoch   4 Step:     3300 Batch Loss:     2.987491 Tokens per Sec:    20482, Lr: 0.000300\n",
            "2020-06-03 20:30:32,289 Epoch   4 Step:     3400 Batch Loss:     3.040964 Tokens per Sec:    20936, Lr: 0.000300\n",
            "2020-06-03 20:30:42,915 Epoch   4 Step:     3500 Batch Loss:     3.065632 Tokens per Sec:    21055, Lr: 0.000300\n",
            "2020-06-03 20:30:53,399 Epoch   4 Step:     3600 Batch Loss:     2.332494 Tokens per Sec:    20681, Lr: 0.000300\n",
            "2020-06-03 20:31:03,942 Epoch   4 Step:     3700 Batch Loss:     2.682744 Tokens per Sec:    20638, Lr: 0.000300\n",
            "2020-06-03 20:31:14,533 Epoch   4 Step:     3800 Batch Loss:     2.897149 Tokens per Sec:    20181, Lr: 0.000300\n",
            "2020-06-03 20:31:25,093 Epoch   4 Step:     3900 Batch Loss:     3.295926 Tokens per Sec:    20508, Lr: 0.000300\n",
            "2020-06-03 20:31:31,978 Epoch   4: total training loss 2852.39\n",
            "2020-06-03 20:31:31,978 EPOCH 5\n",
            "2020-06-03 20:31:35,630 Epoch   5 Step:     4000 Batch Loss:     2.969199 Tokens per Sec:    20296, Lr: 0.000300\n",
            "2020-06-03 20:31:47,495 Hooray! New best validation result [ppl]!\n",
            "2020-06-03 20:31:47,495 Saving new checkpoint.\n",
            "2020-06-03 20:31:47,727 Example #0\n",
            "2020-06-03 20:31:47,728 \tSource:     What motivated Moses to live as he did ?\n",
            "2020-06-03 20:31:47,728 \tReference:  Duzu ati a Mosisi kpale kɛ ɔbabɔ ye ɛbɛla kɛmɔ yɛha nwolɛ edwɛkɛ la ɛ ?\n",
            "2020-06-03 20:31:47,728 \tHypothesis: Duzu a Mosisi yɛle a ?\n",
            "2020-06-03 20:31:47,728 Example #1\n",
            "2020-06-03 20:31:47,729 \tSource:     Writing Committee\n",
            "2020-06-03 20:31:47,729 \tReference:  Mbulukuhɛlɛlɛ Kɔmatii\n",
            "2020-06-03 20:31:47,729 \tHypothesis: Bɛdabɛ Mɔɔ Bɛyɛ La\n",
            "2020-06-03 20:31:47,730 Example #2\n",
            "2020-06-03 20:31:47,730 \tSource:     He was simply expressing a realistic view of how life in this imperfect world turns out .\n",
            "2020-06-03 20:31:47,730 \tReference:  Mɔɔ kɔ zo wɔ ewiade ɛtane ɛhye anu la anwo edwɛkɛ yɛɛ ɛnee ɔlɛka a .\n",
            "2020-06-03 20:31:47,730 \tHypothesis: Ɛnee ɔdwenle kɛzi ewiade ne anu la anwo .\n",
            "2020-06-03 20:31:47,730 Example #3\n",
            "2020-06-03 20:31:47,730 \tSource:     From the start , Jacob was in love with his beautiful Rachel .\n",
            "2020-06-03 20:31:47,730 \tReference:  Mɔlebɛbo ne , ɛnee Gyekɔbo kulo Relahyɛle mɔɔ anwo yɛ ye fɛ la .\n",
            "2020-06-03 20:31:47,730 \tHypothesis: Mɔɔ Dwosefi yɛle la , ɛnee Sɔɔlo kulo kɛ ɔgya ye .\n",
            "2020-06-03 20:31:47,731 Validation result (greedy) at epoch   5, step     4000: bleu:   7.83, loss: 62694.3828, ppl:  13.8176, duration: 12.0998s\n",
            "2020-06-03 20:31:58,234 Epoch   5 Step:     4100 Batch Loss:     1.836577 Tokens per Sec:    20800, Lr: 0.000300\n",
            "2020-06-03 20:32:08,759 Epoch   5 Step:     4200 Batch Loss:     2.544596 Tokens per Sec:    20419, Lr: 0.000300\n",
            "2020-06-03 20:32:19,362 Epoch   5 Step:     4300 Batch Loss:     2.706964 Tokens per Sec:    20812, Lr: 0.000300\n",
            "2020-06-03 20:32:30,019 Epoch   5 Step:     4400 Batch Loss:     2.465625 Tokens per Sec:    20586, Lr: 0.000300\n",
            "2020-06-03 20:32:40,658 Epoch   5 Step:     4500 Batch Loss:     3.283463 Tokens per Sec:    20962, Lr: 0.000300\n",
            "2020-06-03 20:32:51,188 Epoch   5 Step:     4600 Batch Loss:     3.018209 Tokens per Sec:    20719, Lr: 0.000300\n",
            "2020-06-03 20:33:01,786 Epoch   5 Step:     4700 Batch Loss:     2.534119 Tokens per Sec:    21041, Lr: 0.000300\n",
            "2020-06-03 20:33:12,316 Epoch   5 Step:     4800 Batch Loss:     2.820098 Tokens per Sec:    21230, Lr: 0.000300\n",
            "2020-06-03 20:33:22,882 Epoch   5 Step:     4900 Batch Loss:     2.655108 Tokens per Sec:    20483, Lr: 0.000300\n",
            "2020-06-03 20:33:28,431 Epoch   5: total training loss 2652.53\n",
            "2020-06-03 20:33:28,431 EPOCH 6\n",
            "2020-06-03 20:33:33,500 Epoch   6 Step:     5000 Batch Loss:     2.531547 Tokens per Sec:    20691, Lr: 0.000300\n",
            "2020-06-03 20:33:46,179 Hooray! New best validation result [ppl]!\n",
            "2020-06-03 20:33:46,179 Saving new checkpoint.\n",
            "2020-06-03 20:33:46,440 Example #0\n",
            "2020-06-03 20:33:46,440 \tSource:     What motivated Moses to live as he did ?\n",
            "2020-06-03 20:33:46,440 \tReference:  Duzu ati a Mosisi kpale kɛ ɔbabɔ ye ɛbɛla kɛmɔ yɛha nwolɛ edwɛkɛ la ɛ ?\n",
            "2020-06-03 20:33:46,440 \tHypothesis: Duzu a Mosisi vale ye ngoane rale a ?\n",
            "2020-06-03 20:33:46,440 Example #1\n",
            "2020-06-03 20:33:46,441 \tSource:     Writing Committee\n",
            "2020-06-03 20:33:46,441 \tReference:  Mbulukuhɛlɛlɛ Kɔmatii\n",
            "2020-06-03 20:33:46,441 \tHypothesis: Bɛyɛ Bɛ Nwo\n",
            "2020-06-03 20:33:46,441 Example #2\n",
            "2020-06-03 20:33:46,441 \tSource:     He was simply expressing a realistic view of how life in this imperfect world turns out .\n",
            "2020-06-03 20:33:46,441 \tReference:  Mɔɔ kɔ zo wɔ ewiade ɛtane ɛhye anu la anwo edwɛkɛ yɛɛ ɛnee ɔlɛka a .\n",
            "2020-06-03 20:33:46,441 \tHypothesis: Ɛnee ɔdwenle kɛzi ewiade ɛhye anu la anwo .\n",
            "2020-06-03 20:33:46,441 Example #3\n",
            "2020-06-03 20:33:46,442 \tSource:     From the start , Jacob was in love with his beautiful Rachel .\n",
            "2020-06-03 20:33:46,442 \tReference:  Mɔlebɛbo ne , ɛnee Gyekɔbo kulo Relahyɛle mɔɔ anwo yɛ ye fɛ la .\n",
            "2020-06-03 20:33:46,442 \tHypothesis: Mɔɔ Gyekɔbo yɛle la , ɛnee ɔ nee ɔ gɔnwo mɔ kulo ye .\n",
            "2020-06-03 20:33:46,442 Validation result (greedy) at epoch   6, step     5000: bleu:   9.63, loss: 59083.4102, ppl:  11.8781, duration: 12.9420s\n",
            "2020-06-03 20:33:57,003 Epoch   6 Step:     5100 Batch Loss:     2.940476 Tokens per Sec:    20620, Lr: 0.000300\n",
            "2020-06-03 20:34:07,507 Epoch   6 Step:     5200 Batch Loss:     2.589279 Tokens per Sec:    20866, Lr: 0.000300\n",
            "2020-06-03 20:34:18,020 Epoch   6 Step:     5300 Batch Loss:     2.772437 Tokens per Sec:    20181, Lr: 0.000300\n",
            "2020-06-03 20:34:28,709 Epoch   6 Step:     5400 Batch Loss:     2.308907 Tokens per Sec:    21134, Lr: 0.000300\n",
            "2020-06-03 20:34:39,189 Epoch   6 Step:     5500 Batch Loss:     2.236776 Tokens per Sec:    20792, Lr: 0.000300\n",
            "2020-06-03 20:34:49,737 Epoch   6 Step:     5600 Batch Loss:     2.209727 Tokens per Sec:    20927, Lr: 0.000300\n",
            "2020-06-03 20:35:00,418 Epoch   6 Step:     5700 Batch Loss:     2.507112 Tokens per Sec:    21130, Lr: 0.000300\n",
            "2020-06-03 20:35:11,031 Epoch   6 Step:     5800 Batch Loss:     2.356391 Tokens per Sec:    20536, Lr: 0.000300\n",
            "2020-06-03 20:35:21,694 Epoch   6 Step:     5900 Batch Loss:     2.641431 Tokens per Sec:    20751, Lr: 0.000300\n",
            "2020-06-03 20:35:25,929 Epoch   6: total training loss 2524.54\n",
            "2020-06-03 20:35:25,929 EPOCH 7\n",
            "2020-06-03 20:35:32,267 Epoch   7 Step:     6000 Batch Loss:     3.130558 Tokens per Sec:    20251, Lr: 0.000300\n",
            "2020-06-03 20:35:42,819 Hooray! New best validation result [ppl]!\n",
            "2020-06-03 20:35:42,819 Saving new checkpoint.\n",
            "2020-06-03 20:35:43,036 Example #0\n",
            "2020-06-03 20:35:43,036 \tSource:     What motivated Moses to live as he did ?\n",
            "2020-06-03 20:35:43,036 \tReference:  Duzu ati a Mosisi kpale kɛ ɔbabɔ ye ɛbɛla kɛmɔ yɛha nwolɛ edwɛkɛ la ɛ ?\n",
            "2020-06-03 20:35:43,037 \tHypothesis: Duzu a Mosisi yɛle a ?\n",
            "2020-06-03 20:35:43,037 Example #1\n",
            "2020-06-03 20:35:43,037 \tSource:     Writing Committee\n",
            "2020-06-03 20:35:43,037 \tReference:  Mbulukuhɛlɛlɛ Kɔmatii\n",
            "2020-06-03 20:35:43,037 \tHypothesis: Bɛyɛ Bɛyɛ Bɛ Nwo\n",
            "2020-06-03 20:35:43,037 Example #2\n",
            "2020-06-03 20:35:43,037 \tSource:     He was simply expressing a realistic view of how life in this imperfect world turns out .\n",
            "2020-06-03 20:35:43,037 \tReference:  Mɔɔ kɔ zo wɔ ewiade ɛtane ɛhye anu la anwo edwɛkɛ yɛɛ ɛnee ɔlɛka a .\n",
            "2020-06-03 20:35:43,037 \tHypothesis: Ɔvale ye adwenle ɔziele ewiade ɛhye mɔɔ bɛnze ye la azo .\n",
            "2020-06-03 20:35:43,037 Example #3\n",
            "2020-06-03 20:35:43,038 \tSource:     From the start , Jacob was in love with his beautiful Rachel .\n",
            "2020-06-03 20:35:43,038 \tReference:  Mɔlebɛbo ne , ɛnee Gyekɔbo kulo Relahyɛle mɔɔ anwo yɛ ye fɛ la .\n",
            "2020-06-03 20:35:43,038 \tHypothesis: Mɔɔ Gyekɔbo yɛle la , ɛnee ɔ nee ɔ gɔnwo mɔ lɛ ɛlɔlɛ kpole .\n",
            "2020-06-03 20:35:43,038 Validation result (greedy) at epoch   7, step     6000: bleu:  11.37, loss: 56475.4766, ppl:  10.6490, duration: 10.7707s\n",
            "2020-06-03 20:35:53,558 Epoch   7 Step:     6100 Batch Loss:     2.508629 Tokens per Sec:    20459, Lr: 0.000300\n",
            "2020-06-03 20:36:04,182 Epoch   7 Step:     6200 Batch Loss:     2.913216 Tokens per Sec:    21064, Lr: 0.000300\n",
            "2020-06-03 20:36:14,572 Epoch   7 Step:     6300 Batch Loss:     2.606669 Tokens per Sec:    20373, Lr: 0.000300\n",
            "2020-06-03 20:36:25,186 Epoch   7 Step:     6400 Batch Loss:     2.437153 Tokens per Sec:    20764, Lr: 0.000300\n",
            "2020-06-03 20:36:35,770 Epoch   7 Step:     6500 Batch Loss:     3.009320 Tokens per Sec:    20302, Lr: 0.000300\n",
            "2020-06-03 20:36:46,366 Epoch   7 Step:     6600 Batch Loss:     2.411034 Tokens per Sec:    20595, Lr: 0.000300\n",
            "2020-06-03 20:36:57,026 Epoch   7 Step:     6700 Batch Loss:     2.456742 Tokens per Sec:    20743, Lr: 0.000300\n",
            "2020-06-03 20:37:07,546 Epoch   7 Step:     6800 Batch Loss:     2.615068 Tokens per Sec:    20980, Lr: 0.000300\n",
            "2020-06-03 20:37:18,135 Epoch   7 Step:     6900 Batch Loss:     2.469767 Tokens per Sec:    20611, Lr: 0.000300\n",
            "2020-06-03 20:37:21,716 Epoch   7: total training loss 2439.29\n",
            "2020-06-03 20:37:21,716 EPOCH 8\n",
            "2020-06-03 20:37:28,696 Epoch   8 Step:     7000 Batch Loss:     2.714288 Tokens per Sec:    19770, Lr: 0.000300\n",
            "2020-06-03 20:37:40,443 Hooray! New best validation result [ppl]!\n",
            "2020-06-03 20:37:40,443 Saving new checkpoint.\n",
            "2020-06-03 20:37:40,665 Example #0\n",
            "2020-06-03 20:37:40,666 \tSource:     What motivated Moses to live as he did ?\n",
            "2020-06-03 20:37:40,666 \tReference:  Duzu ati a Mosisi kpale kɛ ɔbabɔ ye ɛbɛla kɛmɔ yɛha nwolɛ edwɛkɛ la ɛ ?\n",
            "2020-06-03 20:37:40,666 \tHypothesis: Duzu a Mosisi vale ɔ nwo dole ɛkɛ a ?\n",
            "2020-06-03 20:37:40,666 Example #1\n",
            "2020-06-03 20:37:40,666 \tSource:     Writing Committee\n",
            "2020-06-03 20:37:40,666 \tReference:  Mbulukuhɛlɛlɛ Kɔmatii\n",
            "2020-06-03 20:37:40,666 \tHypothesis: Bɛva Bɛ Nwo Ɛtane\n",
            "2020-06-03 20:37:40,666 Example #2\n",
            "2020-06-03 20:37:40,666 \tSource:     He was simply expressing a realistic view of how life in this imperfect world turns out .\n",
            "2020-06-03 20:37:40,667 \tReference:  Mɔɔ kɔ zo wɔ ewiade ɛtane ɛhye anu la anwo edwɛkɛ yɛɛ ɛnee ɔlɛka a .\n",
            "2020-06-03 20:37:40,667 \tHypothesis: Ɔvale ɔ nwo ɔdole nu kɛ ɔbu ewiade ɛhye mɔɔ ɛnli munli la .\n",
            "2020-06-03 20:37:40,667 Example #3\n",
            "2020-06-03 20:37:40,667 \tSource:     From the start , Jacob was in love with his beautiful Rachel .\n",
            "2020-06-03 20:37:40,667 \tReference:  Mɔlebɛbo ne , ɛnee Gyekɔbo kulo Relahyɛle mɔɔ anwo yɛ ye fɛ la .\n",
            "2020-06-03 20:37:40,667 \tHypothesis: Mɔɔ Gyekɔbo bɔle ɔ bo la , ɛnee ɔ nee ɔ gɔnwo mɔ kulo kɛ ɔnea ye .\n",
            "2020-06-03 20:37:40,667 Validation result (greedy) at epoch   8, step     7000: bleu:  12.22, loss: 53976.7070, ppl:   9.5908, duration: 11.9710s\n",
            "2020-06-03 20:37:51,269 Epoch   8 Step:     7100 Batch Loss:     1.590563 Tokens per Sec:    20720, Lr: 0.000300\n",
            "2020-06-03 20:38:01,823 Epoch   8 Step:     7200 Batch Loss:     2.484280 Tokens per Sec:    20863, Lr: 0.000300\n",
            "2020-06-03 20:38:12,397 Epoch   8 Step:     7300 Batch Loss:     2.134559 Tokens per Sec:    21024, Lr: 0.000300\n",
            "2020-06-03 20:38:23,040 Epoch   8 Step:     7400 Batch Loss:     2.632799 Tokens per Sec:    20970, Lr: 0.000300\n",
            "2020-06-03 20:38:33,672 Epoch   8 Step:     7500 Batch Loss:     2.508469 Tokens per Sec:    20834, Lr: 0.000300\n",
            "2020-06-03 20:38:44,265 Epoch   8 Step:     7600 Batch Loss:     2.659339 Tokens per Sec:    20766, Lr: 0.000300\n",
            "2020-06-03 20:38:54,771 Epoch   8 Step:     7700 Batch Loss:     2.376364 Tokens per Sec:    20758, Lr: 0.000300\n",
            "2020-06-03 20:39:05,260 Epoch   8 Step:     7800 Batch Loss:     2.910115 Tokens per Sec:    20991, Lr: 0.000300\n",
            "2020-06-03 20:39:15,867 Epoch   8 Step:     7900 Batch Loss:     2.852696 Tokens per Sec:    20408, Lr: 0.000300\n",
            "2020-06-03 20:39:18,365 Epoch   8: total training loss 2344.54\n",
            "2020-06-03 20:39:18,365 EPOCH 9\n",
            "2020-06-03 20:39:26,482 Epoch   9 Step:     8000 Batch Loss:     2.321923 Tokens per Sec:    20018, Lr: 0.000300\n",
            "2020-06-03 20:39:41,197 Hooray! New best validation result [ppl]!\n",
            "2020-06-03 20:39:41,197 Saving new checkpoint.\n",
            "2020-06-03 20:39:41,436 Example #0\n",
            "2020-06-03 20:39:41,436 \tSource:     What motivated Moses to live as he did ?\n",
            "2020-06-03 20:39:41,436 \tReference:  Duzu ati a Mosisi kpale kɛ ɔbabɔ ye ɛbɛla kɛmɔ yɛha nwolɛ edwɛkɛ la ɛ ?\n",
            "2020-06-03 20:39:41,436 \tHypothesis: Duzu a Mosisi yɛle ye wɔ ye ɛbɛlabɔlɛ nu a ?\n",
            "2020-06-03 20:39:41,436 Example #1\n",
            "2020-06-03 20:39:41,437 \tSource:     Writing Committee\n",
            "2020-06-03 20:39:41,437 \tReference:  Mbulukuhɛlɛlɛ Kɔmatii\n",
            "2020-06-03 20:39:41,437 \tHypothesis: Bɛyɛ Mɔɔ Bɛyɛ La\n",
            "2020-06-03 20:39:41,437 Example #2\n",
            "2020-06-03 20:39:41,437 \tSource:     He was simply expressing a realistic view of how life in this imperfect world turns out .\n",
            "2020-06-03 20:39:41,437 \tReference:  Mɔɔ kɔ zo wɔ ewiade ɛtane ɛhye anu la anwo edwɛkɛ yɛɛ ɛnee ɔlɛka a .\n",
            "2020-06-03 20:39:41,437 \tHypothesis: Ɛnee ɔle kɛ ɔbaha kɛzi ewiade ɛhye anu amra mɔɔ ɛnli munli la anwo edwɛkɛ .\n",
            "2020-06-03 20:39:41,437 Example #3\n",
            "2020-06-03 20:39:41,437 \tSource:     From the start , Jacob was in love with his beautiful Rachel .\n",
            "2020-06-03 20:39:41,437 \tReference:  Mɔlebɛbo ne , ɛnee Gyekɔbo kulo Relahyɛle mɔɔ anwo yɛ ye fɛ la .\n",
            "2020-06-03 20:39:41,437 \tHypothesis: Mekɛ mɔɔ Gyekɔbo bɔle ɔ bo la , ɛnee ɔ nee ɔ gɔnwo mɔ kulo kɛ ɔnea ɔ nwo .\n",
            "2020-06-03 20:39:41,438 Validation result (greedy) at epoch   9, step     8000: bleu:  13.26, loss: 51988.8555, ppl:   8.8246, duration: 14.9549s\n",
            "2020-06-03 20:39:52,056 Epoch   9 Step:     8100 Batch Loss:     2.275936 Tokens per Sec:    21001, Lr: 0.000300\n",
            "2020-06-03 20:40:02,694 Epoch   9 Step:     8200 Batch Loss:     2.609250 Tokens per Sec:    20972, Lr: 0.000300\n",
            "2020-06-03 20:40:13,288 Epoch   9 Step:     8300 Batch Loss:     2.471962 Tokens per Sec:    20604, Lr: 0.000300\n",
            "2020-06-03 20:40:23,761 Epoch   9 Step:     8400 Batch Loss:     2.396071 Tokens per Sec:    20574, Lr: 0.000300\n",
            "2020-06-03 20:40:34,320 Epoch   9 Step:     8500 Batch Loss:     2.474287 Tokens per Sec:    21268, Lr: 0.000300\n",
            "2020-06-03 20:40:44,946 Epoch   9 Step:     8600 Batch Loss:     3.063585 Tokens per Sec:    20520, Lr: 0.000300\n",
            "2020-06-03 20:40:55,539 Epoch   9 Step:     8700 Batch Loss:     2.774219 Tokens per Sec:    20887, Lr: 0.000300\n",
            "2020-06-03 20:41:06,081 Epoch   9 Step:     8800 Batch Loss:     2.506316 Tokens per Sec:    20824, Lr: 0.000300\n",
            "2020-06-03 20:41:16,624 Epoch   9 Step:     8900 Batch Loss:     2.337338 Tokens per Sec:    20533, Lr: 0.000300\n",
            "2020-06-03 20:41:17,734 Epoch   9: total training loss 2262.64\n",
            "2020-06-03 20:41:17,735 EPOCH 10\n",
            "2020-06-03 20:41:27,416 Epoch  10 Step:     9000 Batch Loss:     2.915542 Tokens per Sec:    20297, Lr: 0.000300\n",
            "2020-06-03 20:41:38,876 Hooray! New best validation result [ppl]!\n",
            "2020-06-03 20:41:38,876 Saving new checkpoint.\n",
            "2020-06-03 20:41:39,152 Example #0\n",
            "2020-06-03 20:41:39,152 \tSource:     What motivated Moses to live as he did ?\n",
            "2020-06-03 20:41:39,152 \tReference:  Duzu ati a Mosisi kpale kɛ ɔbabɔ ye ɛbɛla kɛmɔ yɛha nwolɛ edwɛkɛ la ɛ ?\n",
            "2020-06-03 20:41:39,152 \tHypothesis: Duzu a Mosisi yɛle a ?\n",
            "2020-06-03 20:41:39,153 Example #1\n",
            "2020-06-03 20:41:39,153 \tSource:     Writing Committee\n",
            "2020-06-03 20:41:39,153 \tReference:  Mbulukuhɛlɛlɛ Kɔmatii\n",
            "2020-06-03 20:41:39,153 \tHypothesis: Bɛyɛ Mɔɔ Bɛyɛ La\n",
            "2020-06-03 20:41:39,153 Example #2\n",
            "2020-06-03 20:41:39,153 \tSource:     He was simply expressing a realistic view of how life in this imperfect world turns out .\n",
            "2020-06-03 20:41:39,153 \tReference:  Mɔɔ kɔ zo wɔ ewiade ɛtane ɛhye anu la anwo edwɛkɛ yɛɛ ɛnee ɔlɛka a .\n",
            "2020-06-03 20:41:39,153 \tHypothesis: Ɛnee ɔnze kɛzi ewiade ɛhye mɔɔ ɛnli munli la .\n",
            "2020-06-03 20:41:39,153 Example #3\n",
            "2020-06-03 20:41:39,153 \tSource:     From the start , Jacob was in love with his beautiful Rachel .\n",
            "2020-06-03 20:41:39,154 \tReference:  Mɔlebɛbo ne , ɛnee Gyekɔbo kulo Relahyɛle mɔɔ anwo yɛ ye fɛ la .\n",
            "2020-06-03 20:41:39,154 \tHypothesis: Wɔ Gyekɔbo nee ɔ gɔnwo mɔ avinli , ɛnee ɔkulo Releehabe kpalɛ .\n",
            "2020-06-03 20:41:39,154 Validation result (greedy) at epoch  10, step     9000: bleu:  14.35, loss: 50456.0859, ppl:   8.2759, duration: 11.7374s\n",
            "2020-06-03 20:41:49,926 Epoch  10 Step:     9100 Batch Loss:     2.010946 Tokens per Sec:    20209, Lr: 0.000300\n",
            "2020-06-03 20:42:00,630 Epoch  10 Step:     9200 Batch Loss:     2.552171 Tokens per Sec:    20625, Lr: 0.000300\n",
            "2020-06-03 20:42:11,301 Epoch  10 Step:     9300 Batch Loss:     2.108702 Tokens per Sec:    20860, Lr: 0.000300\n",
            "2020-06-03 20:42:21,978 Epoch  10 Step:     9400 Batch Loss:     1.969523 Tokens per Sec:    20304, Lr: 0.000300\n",
            "2020-06-03 20:42:32,630 Epoch  10 Step:     9500 Batch Loss:     2.765766 Tokens per Sec:    20670, Lr: 0.000300\n",
            "2020-06-03 20:42:43,088 Epoch  10 Step:     9600 Batch Loss:     2.138626 Tokens per Sec:    20418, Lr: 0.000300\n",
            "2020-06-03 20:42:53,595 Epoch  10 Step:     9700 Batch Loss:     2.056605 Tokens per Sec:    20725, Lr: 0.000300\n",
            "2020-06-03 20:43:04,131 Epoch  10 Step:     9800 Batch Loss:     2.155245 Tokens per Sec:    20461, Lr: 0.000300\n",
            "2020-06-03 20:43:14,656 Epoch  10 Step:     9900 Batch Loss:     2.172149 Tokens per Sec:    20487, Lr: 0.000300\n",
            "2020-06-03 20:43:15,100 Epoch  10: total training loss 2216.12\n",
            "2020-06-03 20:43:15,100 EPOCH 11\n",
            "2020-06-03 20:43:25,369 Epoch  11 Step:    10000 Batch Loss:     2.816561 Tokens per Sec:    20527, Lr: 0.000300\n",
            "2020-06-03 20:43:37,160 Hooray! New best validation result [ppl]!\n",
            "2020-06-03 20:43:37,160 Saving new checkpoint.\n",
            "2020-06-03 20:43:37,395 Example #0\n",
            "2020-06-03 20:43:37,396 \tSource:     What motivated Moses to live as he did ?\n",
            "2020-06-03 20:43:37,396 \tReference:  Duzu ati a Mosisi kpale kɛ ɔbabɔ ye ɛbɛla kɛmɔ yɛha nwolɛ edwɛkɛ la ɛ ?\n",
            "2020-06-03 20:43:37,396 \tHypothesis: Duzu a Mosisi yɛle amaa yeadɛnla aze wɔ mekɛ mɔɔ ɔyɛle la ɛ ?\n",
            "2020-06-03 20:43:37,396 Example #1\n",
            "2020-06-03 20:43:37,396 \tSource:     Writing Committee\n",
            "2020-06-03 20:43:37,396 \tReference:  Mbulukuhɛlɛlɛ Kɔmatii\n",
            "2020-06-03 20:43:37,396 \tHypothesis: Bɛyɛ Bɛyɛ Bɛgua Nu\n",
            "2020-06-03 20:43:37,396 Example #2\n",
            "2020-06-03 20:43:37,396 \tSource:     He was simply expressing a realistic view of how life in this imperfect world turns out .\n",
            "2020-06-03 20:43:37,396 \tReference:  Mɔɔ kɔ zo wɔ ewiade ɛtane ɛhye anu la anwo edwɛkɛ yɛɛ ɛnee ɔlɛka a .\n",
            "2020-06-03 20:43:37,396 \tHypothesis: Ɛnee ɔtɛbɔ kpɔkɛ kɛ ɔbabu kɛzi alesama mɔɔ ɛnli munli la kɛ bɛbadɛnla ewiade ɛhye anu la .\n",
            "2020-06-03 20:43:37,396 Example #3\n",
            "2020-06-03 20:43:37,397 \tSource:     From the start , Jacob was in love with his beautiful Rachel .\n",
            "2020-06-03 20:43:37,397 \tReference:  Mɔlebɛbo ne , ɛnee Gyekɔbo kulo Relahyɛle mɔɔ anwo yɛ ye fɛ la .\n",
            "2020-06-03 20:43:37,397 \tHypothesis: Wɔ Gyekɔbo mumua ne , ɛnee ɔ nee ɔ Ra Rilihɔbowam kulo ye kpalɛ .\n",
            "2020-06-03 20:43:37,397 Validation result (greedy) at epoch  11, step    10000: bleu:  15.41, loss: 49320.1641, ppl:   7.8913, duration: 12.0276s\n",
            "2020-06-03 20:43:47,987 Epoch  11 Step:    10100 Batch Loss:     2.470625 Tokens per Sec:    20519, Lr: 0.000300\n",
            "2020-06-03 20:43:58,660 Epoch  11 Step:    10200 Batch Loss:     2.060580 Tokens per Sec:    20911, Lr: 0.000300\n",
            "2020-06-03 20:44:09,212 Epoch  11 Step:    10300 Batch Loss:     2.210054 Tokens per Sec:    20603, Lr: 0.000300\n",
            "2020-06-03 20:44:19,921 Epoch  11 Step:    10400 Batch Loss:     2.482451 Tokens per Sec:    20571, Lr: 0.000300\n",
            "2020-06-03 20:44:30,467 Epoch  11 Step:    10500 Batch Loss:     2.416322 Tokens per Sec:    20381, Lr: 0.000300\n",
            "2020-06-03 20:44:41,060 Epoch  11 Step:    10600 Batch Loss:     2.048045 Tokens per Sec:    20634, Lr: 0.000300\n",
            "2020-06-03 20:44:51,652 Epoch  11 Step:    10700 Batch Loss:     2.080526 Tokens per Sec:    20956, Lr: 0.000300\n",
            "2020-06-03 20:45:02,254 Epoch  11 Step:    10800 Batch Loss:     1.777888 Tokens per Sec:    20591, Lr: 0.000300\n",
            "2020-06-03 20:45:12,151 Epoch  11: total training loss 2154.45\n",
            "2020-06-03 20:45:12,151 EPOCH 12\n",
            "2020-06-03 20:45:12,893 Epoch  12 Step:    10900 Batch Loss:     2.219637 Tokens per Sec:    18580, Lr: 0.000300\n",
            "2020-06-03 20:45:23,432 Epoch  12 Step:    11000 Batch Loss:     2.149979 Tokens per Sec:    20788, Lr: 0.000300\n",
            "2020-06-03 20:45:34,987 Hooray! New best validation result [ppl]!\n",
            "2020-06-03 20:45:34,988 Saving new checkpoint.\n",
            "2020-06-03 20:45:35,214 Example #0\n",
            "2020-06-03 20:45:35,215 \tSource:     What motivated Moses to live as he did ?\n",
            "2020-06-03 20:45:35,215 \tReference:  Duzu ati a Mosisi kpale kɛ ɔbabɔ ye ɛbɛla kɛmɔ yɛha nwolɛ edwɛkɛ la ɛ ?\n",
            "2020-06-03 20:45:35,215 \tHypothesis: Duzu a Mosisi yɛle a ?\n",
            "2020-06-03 20:45:35,215 Example #1\n",
            "2020-06-03 20:45:35,215 \tSource:     Writing Committee\n",
            "2020-06-03 20:45:35,215 \tReference:  Mbulukuhɛlɛlɛ Kɔmatii\n",
            "2020-06-03 20:45:35,215 \tHypothesis: Bɔ Bɔ Buu\n",
            "2020-06-03 20:45:35,215 Example #2\n",
            "2020-06-03 20:45:35,215 \tSource:     He was simply expressing a realistic view of how life in this imperfect world turns out .\n",
            "2020-06-03 20:45:35,216 \tReference:  Mɔɔ kɔ zo wɔ ewiade ɛtane ɛhye anu la anwo edwɛkɛ yɛɛ ɛnee ɔlɛka a .\n",
            "2020-06-03 20:45:35,216 \tHypothesis: Ɛnee ɔtɛbɔ kpɔkɛ kɛ ɔbabu kɛzi ewiade ɛhye anu amra mɔɔ ɛnli munli la .\n",
            "2020-06-03 20:45:35,216 Example #3\n",
            "2020-06-03 20:45:35,216 \tSource:     From the start , Jacob was in love with his beautiful Rachel .\n",
            "2020-06-03 20:45:35,216 \tReference:  Mɔlebɛbo ne , ɛnee Gyekɔbo kulo Relahyɛle mɔɔ anwo yɛ ye fɛ la .\n",
            "2020-06-03 20:45:35,216 \tHypothesis: Mɔɔ Gyekɔbo bɔle ɔ bo la , ɛnee Gyekɔbo kulo ɔ nwo kpalɛ .\n",
            "2020-06-03 20:45:35,216 Validation result (greedy) at epoch  12, step    11000: bleu:  16.26, loss: 47784.8516, ppl:   7.3999, duration: 11.7833s\n",
            "2020-06-03 20:45:45,825 Epoch  12 Step:    11100 Batch Loss:     2.089591 Tokens per Sec:    20769, Lr: 0.000300\n",
            "2020-06-03 20:45:56,411 Epoch  12 Step:    11200 Batch Loss:     1.969995 Tokens per Sec:    20911, Lr: 0.000300\n",
            "2020-06-03 20:46:06,909 Epoch  12 Step:    11300 Batch Loss:     2.063565 Tokens per Sec:    20759, Lr: 0.000300\n",
            "2020-06-03 20:46:17,463 Epoch  12 Step:    11400 Batch Loss:     1.911064 Tokens per Sec:    20340, Lr: 0.000300\n",
            "2020-06-03 20:46:28,113 Epoch  12 Step:    11500 Batch Loss:     1.935184 Tokens per Sec:    20940, Lr: 0.000300\n",
            "2020-06-03 20:46:38,678 Epoch  12 Step:    11600 Batch Loss:     2.308118 Tokens per Sec:    20669, Lr: 0.000300\n",
            "2020-06-03 20:46:49,361 Epoch  12 Step:    11700 Batch Loss:     1.723787 Tokens per Sec:    21013, Lr: 0.000300\n",
            "2020-06-03 20:46:59,871 Epoch  12 Step:    11800 Batch Loss:     1.952866 Tokens per Sec:    20358, Lr: 0.000300\n",
            "2020-06-03 20:47:08,551 Epoch  12: total training loss 2094.62\n",
            "2020-06-03 20:47:08,551 EPOCH 13\n",
            "2020-06-03 20:47:10,565 Epoch  13 Step:    11900 Batch Loss:     1.705585 Tokens per Sec:    19274, Lr: 0.000300\n",
            "2020-06-03 20:47:21,202 Epoch  13 Step:    12000 Batch Loss:     1.402788 Tokens per Sec:    20826, Lr: 0.000300\n",
            "2020-06-03 20:47:36,272 Hooray! New best validation result [ppl]!\n",
            "2020-06-03 20:47:36,273 Saving new checkpoint.\n",
            "2020-06-03 20:47:36,494 Example #0\n",
            "2020-06-03 20:47:36,495 \tSource:     What motivated Moses to live as he did ?\n",
            "2020-06-03 20:47:36,495 \tReference:  Duzu ati a Mosisi kpale kɛ ɔbabɔ ye ɛbɛla kɛmɔ yɛha nwolɛ edwɛkɛ la ɛ ?\n",
            "2020-06-03 20:47:36,495 \tHypothesis: Duzu a Mosisi yɛle a ?\n",
            "2020-06-03 20:47:36,495 Example #1\n",
            "2020-06-03 20:47:36,495 \tSource:     Writing Committee\n",
            "2020-06-03 20:47:36,495 \tReference:  Mbulukuhɛlɛlɛ Kɔmatii\n",
            "2020-06-03 20:47:36,495 \tHypothesis: Bɔ Buu\n",
            "2020-06-03 20:47:36,495 Example #2\n",
            "2020-06-03 20:47:36,495 \tSource:     He was simply expressing a realistic view of how life in this imperfect world turns out .\n",
            "2020-06-03 20:47:36,495 \tReference:  Mɔɔ kɔ zo wɔ ewiade ɛtane ɛhye anu la anwo edwɛkɛ yɛɛ ɛnee ɔlɛka a .\n",
            "2020-06-03 20:47:36,495 \tHypothesis: Ɛnee ɔtɛbɔ kpɔkɛ kɛ ɔbabu kɛzi ewiade ɛhye anu amra mɔɔ ɛnli munli la .\n",
            "2020-06-03 20:47:36,495 Example #3\n",
            "2020-06-03 20:47:36,496 \tSource:     From the start , Jacob was in love with his beautiful Rachel .\n",
            "2020-06-03 20:47:36,496 \tReference:  Mɔlebɛbo ne , ɛnee Gyekɔbo kulo Relahyɛle mɔɔ anwo yɛ ye fɛ la .\n",
            "2020-06-03 20:47:36,496 \tHypothesis: Mɔɔ Gyekɔbo bɔle ɔ bo la , ɛnee ɔkulo ɔ nee Raaayaayase .\n",
            "2020-06-03 20:47:36,496 Validation result (greedy) at epoch  13, step    12000: bleu:  17.12, loss: 46769.1445, ppl:   7.0916, duration: 15.2938s\n",
            "2020-06-03 20:47:47,114 Epoch  13 Step:    12100 Batch Loss:     1.800990 Tokens per Sec:    20793, Lr: 0.000300\n",
            "2020-06-03 20:47:57,669 Epoch  13 Step:    12200 Batch Loss:     1.364324 Tokens per Sec:    20673, Lr: 0.000300\n",
            "2020-06-03 20:48:08,231 Epoch  13 Step:    12300 Batch Loss:     1.931843 Tokens per Sec:    20752, Lr: 0.000300\n",
            "2020-06-03 20:48:18,729 Epoch  13 Step:    12400 Batch Loss:     2.100173 Tokens per Sec:    20835, Lr: 0.000300\n",
            "2020-06-03 20:48:29,352 Epoch  13 Step:    12500 Batch Loss:     2.326886 Tokens per Sec:    20945, Lr: 0.000300\n",
            "2020-06-03 20:48:40,013 Epoch  13 Step:    12600 Batch Loss:     2.146747 Tokens per Sec:    20572, Lr: 0.000300\n",
            "2020-06-03 20:48:50,543 Epoch  13 Step:    12700 Batch Loss:     2.425299 Tokens per Sec:    20566, Lr: 0.000300\n",
            "2020-06-03 20:49:01,191 Epoch  13 Step:    12800 Batch Loss:     2.192448 Tokens per Sec:    21208, Lr: 0.000300\n",
            "2020-06-03 20:49:08,272 Epoch  13: total training loss 2043.62\n",
            "2020-06-03 20:49:08,272 EPOCH 14\n",
            "2020-06-03 20:49:11,860 Epoch  14 Step:    12900 Batch Loss:     1.985016 Tokens per Sec:    19884, Lr: 0.000300\n",
            "2020-06-03 20:49:22,486 Epoch  14 Step:    13000 Batch Loss:     1.942756 Tokens per Sec:    21184, Lr: 0.000300\n",
            "2020-06-03 20:49:33,823 Hooray! New best validation result [ppl]!\n",
            "2020-06-03 20:49:33,824 Saving new checkpoint.\n",
            "2020-06-03 20:49:34,045 Example #0\n",
            "2020-06-03 20:49:34,045 \tSource:     What motivated Moses to live as he did ?\n",
            "2020-06-03 20:49:34,045 \tReference:  Duzu ati a Mosisi kpale kɛ ɔbabɔ ye ɛbɛla kɛmɔ yɛha nwolɛ edwɛkɛ la ɛ ?\n",
            "2020-06-03 20:49:34,045 \tHypothesis: Duzu a Mosisi yɛle a ?\n",
            "2020-06-03 20:49:34,045 Example #1\n",
            "2020-06-03 20:49:34,046 \tSource:     Writing Committee\n",
            "2020-06-03 20:49:34,046 \tReference:  Mbulukuhɛlɛlɛ Kɔmatii\n",
            "2020-06-03 20:49:34,046 \tHypothesis: Bɛyɛ Bɛyɛ Bɛyɛ Bɛyɛ Bɛ Nwo\n",
            "2020-06-03 20:49:34,046 Example #2\n",
            "2020-06-03 20:49:34,046 \tSource:     He was simply expressing a realistic view of how life in this imperfect world turns out .\n",
            "2020-06-03 20:49:34,046 \tReference:  Mɔɔ kɔ zo wɔ ewiade ɛtane ɛhye anu la anwo edwɛkɛ yɛɛ ɛnee ɔlɛka a .\n",
            "2020-06-03 20:49:34,046 \tHypothesis: Ɛnee ɔtɛnwunle kɛzi ewiade ɛhye anu amra mɔɔ ɛnli munli la .\n",
            "2020-06-03 20:49:34,046 Example #3\n",
            "2020-06-03 20:49:34,046 \tSource:     From the start , Jacob was in love with his beautiful Rachel .\n",
            "2020-06-03 20:49:34,046 \tReference:  Mɔlebɛbo ne , ɛnee Gyekɔbo kulo Relahyɛle mɔɔ anwo yɛ ye fɛ la .\n",
            "2020-06-03 20:49:34,047 \tHypothesis: Ɔvi mɔlebɛbo ne , Gyekɔbo nee ɔ gɔnwo Rachel yɛle yɛle kɛnlɛma .\n",
            "2020-06-03 20:49:34,047 Validation result (greedy) at epoch  14, step    13000: bleu:  17.96, loss: 45831.4297, ppl:   6.8185, duration: 11.5602s\n",
            "2020-06-03 20:49:44,597 Epoch  14 Step:    13100 Batch Loss:     2.224807 Tokens per Sec:    20733, Lr: 0.000300\n",
            "2020-06-03 20:49:55,096 Epoch  14 Step:    13200 Batch Loss:     1.994904 Tokens per Sec:    20394, Lr: 0.000300\n",
            "2020-06-03 20:50:05,662 Epoch  14 Step:    13300 Batch Loss:     1.618733 Tokens per Sec:    20658, Lr: 0.000300\n",
            "2020-06-03 20:50:16,277 Epoch  14 Step:    13400 Batch Loss:     2.117444 Tokens per Sec:    20762, Lr: 0.000300\n",
            "2020-06-03 20:50:26,954 Epoch  14 Step:    13500 Batch Loss:     2.780988 Tokens per Sec:    21007, Lr: 0.000300\n",
            "2020-06-03 20:50:37,571 Epoch  14 Step:    13600 Batch Loss:     2.007619 Tokens per Sec:    20647, Lr: 0.000300\n",
            "2020-06-03 20:50:48,208 Epoch  14 Step:    13700 Batch Loss:     1.865649 Tokens per Sec:    20722, Lr: 0.000300\n",
            "2020-06-03 20:50:58,604 Epoch  14 Step:    13800 Batch Loss:     1.815150 Tokens per Sec:    20242, Lr: 0.000300\n",
            "2020-06-03 20:51:04,372 Epoch  14: total training loss 2007.52\n",
            "2020-06-03 20:51:04,372 EPOCH 15\n",
            "2020-06-03 20:51:09,391 Epoch  15 Step:    13900 Batch Loss:     1.451293 Tokens per Sec:    21130, Lr: 0.000300\n",
            "2020-06-03 20:51:19,983 Epoch  15 Step:    14000 Batch Loss:     2.288198 Tokens per Sec:    20800, Lr: 0.000300\n",
            "2020-06-03 20:51:32,715 Hooray! New best validation result [ppl]!\n",
            "2020-06-03 20:51:32,715 Saving new checkpoint.\n",
            "2020-06-03 20:51:32,940 Example #0\n",
            "2020-06-03 20:51:32,940 \tSource:     What motivated Moses to live as he did ?\n",
            "2020-06-03 20:51:32,940 \tReference:  Duzu ati a Mosisi kpale kɛ ɔbabɔ ye ɛbɛla kɛmɔ yɛha nwolɛ edwɛkɛ la ɛ ?\n",
            "2020-06-03 20:51:32,940 \tHypothesis: Duzu a manle Mosisi dele nganeɛ wɔ mekɛ mɔɔ ɔyɛle la ɛ ?\n",
            "2020-06-03 20:51:32,940 Example #1\n",
            "2020-06-03 20:51:32,941 \tSource:     Writing Committee\n",
            "2020-06-03 20:51:32,941 \tReference:  Mbulukuhɛlɛlɛ Kɔmatii\n",
            "2020-06-03 20:51:32,941 \tHypothesis: Kɔ Kɔmatii\n",
            "2020-06-03 20:51:32,941 Example #2\n",
            "2020-06-03 20:51:32,941 \tSource:     He was simply expressing a realistic view of how life in this imperfect world turns out .\n",
            "2020-06-03 20:51:32,941 \tReference:  Mɔɔ kɔ zo wɔ ewiade ɛtane ɛhye anu la anwo edwɛkɛ yɛɛ ɛnee ɔlɛka a .\n",
            "2020-06-03 20:51:32,941 \tHypothesis: Ɛnee ɔlɛfa ninyɛne mɔɔ di munli wɔ ewiade ɛhye anu la yeado ɔ nwo zo .\n",
            "2020-06-03 20:51:32,941 Example #3\n",
            "2020-06-03 20:51:32,941 \tSource:     From the start , Jacob was in love with his beautiful Rachel .\n",
            "2020-06-03 20:51:32,941 \tReference:  Mɔlebɛbo ne , ɛnee Gyekɔbo kulo Relahyɛle mɔɔ anwo yɛ ye fɛ la .\n",
            "2020-06-03 20:51:32,941 \tHypothesis: Wɔ mɔlebɛbo ne , Gyekɔbo nee Rachel nyianle ɛlɔlɛ kpole .\n",
            "2020-06-03 20:51:32,941 Validation result (greedy) at epoch  15, step    14000: bleu:  18.06, loss: 45035.7070, ppl:   6.5950, duration: 12.9585s\n",
            "2020-06-03 20:51:43,544 Epoch  15 Step:    14100 Batch Loss:     1.287951 Tokens per Sec:    20465, Lr: 0.000300\n",
            "2020-06-03 20:51:54,123 Epoch  15 Step:    14200 Batch Loss:     2.443581 Tokens per Sec:    20438, Lr: 0.000300\n",
            "2020-06-03 20:52:04,819 Epoch  15 Step:    14300 Batch Loss:     1.394980 Tokens per Sec:    20542, Lr: 0.000300\n",
            "2020-06-03 20:52:15,459 Epoch  15 Step:    14400 Batch Loss:     2.082040 Tokens per Sec:    21002, Lr: 0.000300\n",
            "2020-06-03 20:52:25,993 Epoch  15 Step:    14500 Batch Loss:     1.799919 Tokens per Sec:    20623, Lr: 0.000300\n",
            "2020-06-03 20:52:36,578 Epoch  15 Step:    14600 Batch Loss:     2.415559 Tokens per Sec:    20816, Lr: 0.000300\n",
            "2020-06-03 20:52:47,134 Epoch  15 Step:    14700 Batch Loss:     2.147053 Tokens per Sec:    20615, Lr: 0.000300\n",
            "2020-06-03 20:52:57,663 Epoch  15 Step:    14800 Batch Loss:     2.018322 Tokens per Sec:    20781, Lr: 0.000300\n",
            "2020-06-03 20:53:02,009 Epoch  15: total training loss 1969.06\n",
            "2020-06-03 20:53:02,009 EPOCH 16\n",
            "2020-06-03 20:53:08,337 Epoch  16 Step:    14900 Batch Loss:     2.642159 Tokens per Sec:    20321, Lr: 0.000300\n",
            "2020-06-03 20:53:18,910 Epoch  16 Step:    15000 Batch Loss:     2.743541 Tokens per Sec:    20910, Lr: 0.000300\n",
            "2020-06-03 20:53:33,412 Hooray! New best validation result [ppl]!\n",
            "2020-06-03 20:53:33,412 Saving new checkpoint.\n",
            "2020-06-03 20:53:33,632 Example #0\n",
            "2020-06-03 20:53:33,633 \tSource:     What motivated Moses to live as he did ?\n",
            "2020-06-03 20:53:33,633 \tReference:  Duzu ati a Mosisi kpale kɛ ɔbabɔ ye ɛbɛla kɛmɔ yɛha nwolɛ edwɛkɛ la ɛ ?\n",
            "2020-06-03 20:53:33,633 \tHypothesis: Duzu a manle Mosisi dele nganeɛ kɛ ɔyɛle a ?\n",
            "2020-06-03 20:53:33,633 Example #1\n",
            "2020-06-03 20:53:33,633 \tSource:     Writing Committee\n",
            "2020-06-03 20:53:33,633 \tReference:  Mbulukuhɛlɛlɛ Kɔmatii\n",
            "2020-06-03 20:53:33,633 \tHypothesis: Bɛva Bɛli Adehilelɛ Nwo\n",
            "2020-06-03 20:53:33,633 Example #2\n",
            "2020-06-03 20:53:33,634 \tSource:     He was simply expressing a realistic view of how life in this imperfect world turns out .\n",
            "2020-06-03 20:53:33,634 \tReference:  Mɔɔ kɔ zo wɔ ewiade ɛtane ɛhye anu la anwo edwɛkɛ yɛɛ ɛnee ɔlɛka a .\n",
            "2020-06-03 20:53:33,634 \tHypothesis: Ɛnee ɔle adwenle mɔɔ ɔlɛ ye wɔ ewiade ɛhye anu la .\n",
            "2020-06-03 20:53:33,634 Example #3\n",
            "2020-06-03 20:53:33,634 \tSource:     From the start , Jacob was in love with his beautiful Rachel .\n",
            "2020-06-03 20:53:33,634 \tReference:  Mɔlebɛbo ne , ɛnee Gyekɔbo kulo Relahyɛle mɔɔ anwo yɛ ye fɛ la .\n",
            "2020-06-03 20:53:33,634 \tHypothesis: Ɔvi mɔlebɛbo ne , Gyekɔbo nee ɔ gɔnwo Rachel nyianle ɛlɔlɛ kpole .\n",
            "2020-06-03 20:53:33,634 Validation result (greedy) at epoch  16, step    15000: bleu:  18.91, loss: 44286.4258, ppl:   6.3912, duration: 14.7241s\n",
            "2020-06-03 20:53:44,155 Epoch  16 Step:    15100 Batch Loss:     1.349364 Tokens per Sec:    20306, Lr: 0.000300\n",
            "2020-06-03 20:53:54,824 Epoch  16 Step:    15200 Batch Loss:     2.113323 Tokens per Sec:    20990, Lr: 0.000300\n",
            "2020-06-03 20:54:05,423 Epoch  16 Step:    15300 Batch Loss:     1.498526 Tokens per Sec:    20800, Lr: 0.000300\n",
            "2020-06-03 20:54:15,977 Epoch  16 Step:    15400 Batch Loss:     2.249680 Tokens per Sec:    20684, Lr: 0.000300\n",
            "2020-06-03 20:54:26,520 Epoch  16 Step:    15500 Batch Loss:     1.894716 Tokens per Sec:    20362, Lr: 0.000300\n",
            "2020-06-03 20:54:37,081 Epoch  16 Step:    15600 Batch Loss:     2.092367 Tokens per Sec:    20533, Lr: 0.000300\n",
            "2020-06-03 20:54:47,600 Epoch  16 Step:    15700 Batch Loss:     1.178725 Tokens per Sec:    20825, Lr: 0.000300\n",
            "2020-06-03 20:54:58,212 Epoch  16 Step:    15800 Batch Loss:     2.000570 Tokens per Sec:    21151, Lr: 0.000300\n",
            "2020-06-03 20:55:01,490 Epoch  16: total training loss 1949.58\n",
            "2020-06-03 20:55:01,490 EPOCH 17\n",
            "2020-06-03 20:55:08,937 Epoch  17 Step:    15900 Batch Loss:     2.028865 Tokens per Sec:    20513, Lr: 0.000300\n",
            "2020-06-03 20:55:19,525 Epoch  17 Step:    16000 Batch Loss:     1.869002 Tokens per Sec:    20466, Lr: 0.000300\n",
            "2020-06-03 20:55:31,469 Hooray! New best validation result [ppl]!\n",
            "2020-06-03 20:55:31,470 Saving new checkpoint.\n",
            "2020-06-03 20:55:31,696 Example #0\n",
            "2020-06-03 20:55:31,697 \tSource:     What motivated Moses to live as he did ?\n",
            "2020-06-03 20:55:31,697 \tReference:  Duzu ati a Mosisi kpale kɛ ɔbabɔ ye ɛbɛla kɛmɔ yɛha nwolɛ edwɛkɛ la ɛ ?\n",
            "2020-06-03 20:55:31,697 \tHypothesis: Duzu a manle Mosisi dele nganeɛ kɛ ɔyɛle a ?\n",
            "2020-06-03 20:55:31,697 Example #1\n",
            "2020-06-03 20:55:31,697 \tSource:     Writing Committee\n",
            "2020-06-03 20:55:31,697 \tReference:  Mbulukuhɛlɛlɛ Kɔmatii\n",
            "2020-06-03 20:55:31,697 \tHypothesis: Bɛva Bɛwula Amatii\n",
            "2020-06-03 20:55:31,697 Example #2\n",
            "2020-06-03 20:55:31,698 \tSource:     He was simply expressing a realistic view of how life in this imperfect world turns out .\n",
            "2020-06-03 20:55:31,698 \tReference:  Mɔɔ kɔ zo wɔ ewiade ɛtane ɛhye anu la anwo edwɛkɛ yɛɛ ɛnee ɔlɛka a .\n",
            "2020-06-03 20:55:31,698 \tHypothesis: Ɛnee ɔze kɛzi ɛbɛlabɔlɛ nu ninyɛne mɔɔ ɛnli munli la .\n",
            "2020-06-03 20:55:31,698 Example #3\n",
            "2020-06-03 20:55:31,699 \tSource:     From the start , Jacob was in love with his beautiful Rachel .\n",
            "2020-06-03 20:55:31,699 \tReference:  Mɔlebɛbo ne , ɛnee Gyekɔbo kulo Relahyɛle mɔɔ anwo yɛ ye fɛ la .\n",
            "2020-06-03 20:55:31,699 \tHypothesis: Gyekɔbo nyianle ɔ nwo ɛlɔlɛ kpole wɔ ɔ nwo zo .\n",
            "2020-06-03 20:55:31,699 Validation result (greedy) at epoch  17, step    16000: bleu:  19.51, loss: 43750.0742, ppl:   6.2493, duration: 12.1732s\n",
            "2020-06-03 20:55:42,399 Epoch  17 Step:    16100 Batch Loss:     2.592641 Tokens per Sec:    20716, Lr: 0.000300\n",
            "2020-06-03 20:55:53,049 Epoch  17 Step:    16200 Batch Loss:     1.356684 Tokens per Sec:    20858, Lr: 0.000300\n",
            "2020-06-03 20:56:03,725 Epoch  17 Step:    16300 Batch Loss:     1.707586 Tokens per Sec:    20816, Lr: 0.000300\n",
            "2020-06-03 20:56:14,256 Epoch  17 Step:    16400 Batch Loss:     1.912608 Tokens per Sec:    20325, Lr: 0.000300\n",
            "2020-06-03 20:56:24,929 Epoch  17 Step:    16500 Batch Loss:     2.357255 Tokens per Sec:    20390, Lr: 0.000300\n",
            "2020-06-03 20:56:35,507 Epoch  17 Step:    16600 Batch Loss:     2.113883 Tokens per Sec:    20600, Lr: 0.000300\n",
            "2020-06-03 20:56:46,144 Epoch  17 Step:    16700 Batch Loss:     1.862821 Tokens per Sec:    20951, Lr: 0.000300\n",
            "2020-06-03 20:56:56,711 Epoch  17 Step:    16800 Batch Loss:     1.501345 Tokens per Sec:    20747, Lr: 0.000300\n",
            "2020-06-03 20:56:58,616 Epoch  17: total training loss 1906.54\n",
            "2020-06-03 20:56:58,616 EPOCH 18\n",
            "2020-06-03 20:57:07,356 Epoch  18 Step:    16900 Batch Loss:     1.975717 Tokens per Sec:    20878, Lr: 0.000300\n",
            "2020-06-03 20:57:17,981 Epoch  18 Step:    17000 Batch Loss:     2.534294 Tokens per Sec:    20529, Lr: 0.000300\n",
            "2020-06-03 20:57:30,867 Hooray! New best validation result [ppl]!\n",
            "2020-06-03 20:57:30,868 Saving new checkpoint.\n",
            "2020-06-03 20:57:31,091 Example #0\n",
            "2020-06-03 20:57:31,091 \tSource:     What motivated Moses to live as he did ?\n",
            "2020-06-03 20:57:31,092 \tReference:  Duzu ati a Mosisi kpale kɛ ɔbabɔ ye ɛbɛla kɛmɔ yɛha nwolɛ edwɛkɛ la ɛ ?\n",
            "2020-06-03 20:57:31,092 \tHypothesis: Duzu a manle Mosisi nyianle ngoane wɔ mekɛ mɔɔ ɔyɛle la ɛ ?\n",
            "2020-06-03 20:57:31,092 Example #1\n",
            "2020-06-03 20:57:31,092 \tSource:     Writing Committee\n",
            "2020-06-03 20:57:31,092 \tReference:  Mbulukuhɛlɛlɛ Kɔmatii\n",
            "2020-06-03 20:57:31,092 \tHypothesis: Bɛyɛ Bɛ Nwo Bɛdabɛ Mɔɔ Bɛyɛ La\n",
            "2020-06-03 20:57:31,092 Example #2\n",
            "2020-06-03 20:57:31,092 \tSource:     He was simply expressing a realistic view of how life in this imperfect world turns out .\n",
            "2020-06-03 20:57:31,092 \tReference:  Mɔɔ kɔ zo wɔ ewiade ɛtane ɛhye anu la anwo edwɛkɛ yɛɛ ɛnee ɔlɛka a .\n",
            "2020-06-03 20:57:31,092 \tHypothesis: Ɛnee ɔze kɛzi asetɛnla mɔɔ anu yɛ se la wɔ ewiade ɛhye anu .\n",
            "2020-06-03 20:57:31,092 Example #3\n",
            "2020-06-03 20:57:31,093 \tSource:     From the start , Jacob was in love with his beautiful Rachel .\n",
            "2020-06-03 20:57:31,093 \tReference:  Mɔlebɛbo ne , ɛnee Gyekɔbo kulo Relahyɛle mɔɔ anwo yɛ ye fɛ la .\n",
            "2020-06-03 20:57:31,093 \tHypothesis: Ɔvi mɔlebɛbo ne , Gyekɔbo nyianle ɔ nwo ɛlɔlɛ kpole wɔ Rachel anwo .\n",
            "2020-06-03 20:57:31,093 Validation result (greedy) at epoch  18, step    17000: bleu:  20.01, loss: 43164.3633, ppl:   6.0978, duration: 13.1112s\n",
            "2020-06-03 20:57:41,661 Epoch  18 Step:    17100 Batch Loss:     1.613655 Tokens per Sec:    20556, Lr: 0.000300\n",
            "2020-06-03 20:57:52,285 Epoch  18 Step:    17200 Batch Loss:     1.682419 Tokens per Sec:    20976, Lr: 0.000300\n",
            "2020-06-03 20:58:02,784 Epoch  18 Step:    17300 Batch Loss:     2.042547 Tokens per Sec:    20130, Lr: 0.000300\n",
            "2020-06-03 20:58:13,289 Epoch  18 Step:    17400 Batch Loss:     2.017459 Tokens per Sec:    20970, Lr: 0.000300\n",
            "2020-06-03 20:58:23,919 Epoch  18 Step:    17500 Batch Loss:     1.731880 Tokens per Sec:    20812, Lr: 0.000300\n",
            "2020-06-03 20:58:34,493 Epoch  18 Step:    17600 Batch Loss:     2.164911 Tokens per Sec:    21102, Lr: 0.000300\n",
            "2020-06-03 20:58:44,965 Epoch  18 Step:    17700 Batch Loss:     1.599833 Tokens per Sec:    20273, Lr: 0.000300\n",
            "2020-06-03 20:58:55,670 Epoch  18 Step:    17800 Batch Loss:     1.891374 Tokens per Sec:    21102, Lr: 0.000300\n",
            "2020-06-03 20:58:56,265 Epoch  18: total training loss 1882.52\n",
            "2020-06-03 20:58:56,266 EPOCH 19\n",
            "2020-06-03 20:59:06,277 Epoch  19 Step:    17900 Batch Loss:     2.107935 Tokens per Sec:    20316, Lr: 0.000300\n",
            "2020-06-03 20:59:16,796 Epoch  19 Step:    18000 Batch Loss:     1.497939 Tokens per Sec:    20182, Lr: 0.000300\n",
            "2020-06-03 20:59:30,161 Hooray! New best validation result [ppl]!\n",
            "2020-06-03 20:59:30,161 Saving new checkpoint.\n",
            "2020-06-03 20:59:30,392 Example #0\n",
            "2020-06-03 20:59:30,392 \tSource:     What motivated Moses to live as he did ?\n",
            "2020-06-03 20:59:30,392 \tReference:  Duzu ati a Mosisi kpale kɛ ɔbabɔ ye ɛbɛla kɛmɔ yɛha nwolɛ edwɛkɛ la ɛ ?\n",
            "2020-06-03 20:59:30,392 \tHypothesis: Duzu a manle Mosisi dele nganeɛ kɛ ɔyɛle a ?\n",
            "2020-06-03 20:59:30,392 Example #1\n",
            "2020-06-03 20:59:30,392 \tSource:     Writing Committee\n",
            "2020-06-03 20:59:30,392 \tReference:  Mbulukuhɛlɛlɛ Kɔmatii\n",
            "2020-06-03 20:59:30,392 \tHypothesis: Bɛyɛ Bɛdabɛ Mɔɔ Bɛyɛ La\n",
            "2020-06-03 20:59:30,393 Example #2\n",
            "2020-06-03 20:59:30,393 \tSource:     He was simply expressing a realistic view of how life in this imperfect world turns out .\n",
            "2020-06-03 20:59:30,393 \tReference:  Mɔɔ kɔ zo wɔ ewiade ɛtane ɛhye anu la anwo edwɛkɛ yɛɛ ɛnee ɔlɛka a .\n",
            "2020-06-03 20:59:30,393 \tHypothesis: Ɛnee ɔle debie mɔɔ maa yɛnwu kɛzi asetɛnla mɔɔ di munli wɔ ewiade ɛhye anu la .\n",
            "2020-06-03 20:59:30,393 Example #3\n",
            "2020-06-03 20:59:30,393 \tSource:     From the start , Jacob was in love with his beautiful Rachel .\n",
            "2020-06-03 20:59:30,393 \tReference:  Mɔlebɛbo ne , ɛnee Gyekɔbo kulo Relahyɛle mɔɔ anwo yɛ ye fɛ la .\n",
            "2020-06-03 20:59:30,393 \tHypothesis: Ɔvi mɔlebɛbo ne , Gyekɔbo nyianle ɔ nwo ɛlɔlɛ kpole wɔ Rachel anwo .\n",
            "2020-06-03 20:59:30,393 Validation result (greedy) at epoch  19, step    18000: bleu:  20.05, loss: 42628.8242, ppl:   5.9626, duration: 13.5968s\n",
            "2020-06-03 20:59:40,958 Epoch  19 Step:    18100 Batch Loss:     2.217535 Tokens per Sec:    20773, Lr: 0.000300\n",
            "2020-06-03 20:59:51,528 Epoch  19 Step:    18200 Batch Loss:     1.632673 Tokens per Sec:    20975, Lr: 0.000300\n",
            "2020-06-03 21:00:02,098 Epoch  19 Step:    18300 Batch Loss:     1.987700 Tokens per Sec:    21232, Lr: 0.000300\n",
            "2020-06-03 21:00:12,625 Epoch  19 Step:    18400 Batch Loss:     1.523294 Tokens per Sec:    20978, Lr: 0.000300\n",
            "2020-06-03 21:00:23,187 Epoch  19 Step:    18500 Batch Loss:     1.798734 Tokens per Sec:    20701, Lr: 0.000300\n",
            "2020-06-03 21:00:33,702 Epoch  19 Step:    18600 Batch Loss:     1.912954 Tokens per Sec:    20715, Lr: 0.000300\n",
            "2020-06-03 21:00:44,366 Epoch  19 Step:    18700 Batch Loss:     1.880647 Tokens per Sec:    20870, Lr: 0.000300\n",
            "2020-06-03 21:00:54,362 Epoch  19: total training loss 1855.86\n",
            "2020-06-03 21:00:54,362 EPOCH 20\n",
            "2020-06-03 21:00:55,025 Epoch  20 Step:    18800 Batch Loss:     1.683695 Tokens per Sec:    17995, Lr: 0.000300\n",
            "2020-06-03 21:01:05,489 Epoch  20 Step:    18900 Batch Loss:     2.224637 Tokens per Sec:    20169, Lr: 0.000300\n",
            "2020-06-03 21:01:16,098 Epoch  20 Step:    19000 Batch Loss:     2.071546 Tokens per Sec:    21020, Lr: 0.000300\n",
            "2020-06-03 21:01:28,897 Hooray! New best validation result [ppl]!\n",
            "2020-06-03 21:01:28,898 Saving new checkpoint.\n",
            "2020-06-03 21:01:29,127 Example #0\n",
            "2020-06-03 21:01:29,128 \tSource:     What motivated Moses to live as he did ?\n",
            "2020-06-03 21:01:29,128 \tReference:  Duzu ati a Mosisi kpale kɛ ɔbabɔ ye ɛbɛla kɛmɔ yɛha nwolɛ edwɛkɛ la ɛ ?\n",
            "2020-06-03 21:01:29,128 \tHypothesis: Duzu a manle Mosisi dele nganeɛ kɛ ɔyɛle a ?\n",
            "2020-06-03 21:01:29,128 Example #1\n",
            "2020-06-03 21:01:29,128 \tSource:     Writing Committee\n",
            "2020-06-03 21:01:29,128 \tReference:  Mbulukuhɛlɛlɛ Kɔmatii\n",
            "2020-06-03 21:01:29,128 \tHypothesis: Bɛyɛ Bɛ Nwo Bɛfi Ndetee\n",
            "2020-06-03 21:01:29,128 Example #2\n",
            "2020-06-03 21:01:29,128 \tSource:     He was simply expressing a realistic view of how life in this imperfect world turns out .\n",
            "2020-06-03 21:01:29,128 \tReference:  Mɔɔ kɔ zo wɔ ewiade ɛtane ɛhye anu la anwo edwɛkɛ yɛɛ ɛnee ɔlɛka a .\n",
            "2020-06-03 21:01:29,128 \tHypothesis: Ɛnee ɔze kɛzi ɛbɛlabɔlɛ mɔɔ ɛnli munli la .\n",
            "2020-06-03 21:01:29,128 Example #3\n",
            "2020-06-03 21:01:29,129 \tSource:     From the start , Jacob was in love with his beautiful Rachel .\n",
            "2020-06-03 21:01:29,129 \tReference:  Mɔlebɛbo ne , ɛnee Gyekɔbo kulo Relahyɛle mɔɔ anwo yɛ ye fɛ la .\n",
            "2020-06-03 21:01:29,129 \tHypothesis: Ɔvi mɔlebɛbo ne , Gyekɔbo nee ɔ gɔnwo Rachel hulole ye kpalɛ .\n",
            "2020-06-03 21:01:29,129 Validation result (greedy) at epoch  20, step    19000: bleu:  21.03, loss: 42029.8555, ppl:   5.8148, duration: 13.0301s\n",
            "2020-06-03 21:01:39,786 Epoch  20 Step:    19100 Batch Loss:     1.910251 Tokens per Sec:    20870, Lr: 0.000300\n",
            "2020-06-03 21:01:50,273 Epoch  20 Step:    19200 Batch Loss:     1.649103 Tokens per Sec:    20459, Lr: 0.000300\n",
            "2020-06-03 21:02:00,892 Epoch  20 Step:    19300 Batch Loss:     1.926140 Tokens per Sec:    20869, Lr: 0.000300\n",
            "2020-06-03 21:02:11,443 Epoch  20 Step:    19400 Batch Loss:     1.772982 Tokens per Sec:    20425, Lr: 0.000300\n",
            "2020-06-03 21:02:22,137 Epoch  20 Step:    19500 Batch Loss:     2.083845 Tokens per Sec:    20597, Lr: 0.000300\n",
            "2020-06-03 21:02:32,818 Epoch  20 Step:    19600 Batch Loss:     1.620498 Tokens per Sec:    21009, Lr: 0.000300\n",
            "2020-06-03 21:02:43,413 Epoch  20 Step:    19700 Batch Loss:     2.123952 Tokens per Sec:    21007, Lr: 0.000300\n",
            "2020-06-03 21:02:52,130 Epoch  20: total training loss 1837.52\n",
            "2020-06-03 21:02:52,131 EPOCH 21\n",
            "2020-06-03 21:02:54,082 Epoch  21 Step:    19800 Batch Loss:     1.891639 Tokens per Sec:    20949, Lr: 0.000300\n",
            "2020-06-03 21:03:04,611 Epoch  21 Step:    19900 Batch Loss:     2.138412 Tokens per Sec:    20869, Lr: 0.000300\n",
            "2020-06-03 21:03:15,171 Epoch  21 Step:    20000 Batch Loss:     2.115168 Tokens per Sec:    20846, Lr: 0.000300\n",
            "2020-06-03 21:03:27,404 Hooray! New best validation result [ppl]!\n",
            "2020-06-03 21:03:27,404 Saving new checkpoint.\n",
            "2020-06-03 21:03:27,639 Example #0\n",
            "2020-06-03 21:03:27,640 \tSource:     What motivated Moses to live as he did ?\n",
            "2020-06-03 21:03:27,640 \tReference:  Duzu ati a Mosisi kpale kɛ ɔbabɔ ye ɛbɛla kɛmɔ yɛha nwolɛ edwɛkɛ la ɛ ?\n",
            "2020-06-03 21:03:27,640 \tHypothesis: Duzu a manle Mosisi dele nganeɛ wɔ mekɛ mɔɔ ɔyɛle la ɛ ?\n",
            "2020-06-03 21:03:27,640 Example #1\n",
            "2020-06-03 21:03:27,640 \tSource:     Writing Committee\n",
            "2020-06-03 21:03:27,640 \tReference:  Mbulukuhɛlɛlɛ Kɔmatii\n",
            "2020-06-03 21:03:27,640 \tHypothesis: Bɛyɛ Bɛ Ndetee\n",
            "2020-06-03 21:03:27,640 Example #2\n",
            "2020-06-03 21:03:27,640 \tSource:     He was simply expressing a realistic view of how life in this imperfect world turns out .\n",
            "2020-06-03 21:03:27,640 \tReference:  Mɔɔ kɔ zo wɔ ewiade ɛtane ɛhye anu la anwo edwɛkɛ yɛɛ ɛnee ɔlɛka a .\n",
            "2020-06-03 21:03:27,640 \tHypothesis: Ɛnee ɔze kɛzi ɛbɛlabɔlɛ nu ninyɛne mɔɔ ɛnli munli la wɔ ewiade ɛhye anu .\n",
            "2020-06-03 21:03:27,641 Example #3\n",
            "2020-06-03 21:03:27,641 \tSource:     From the start , Jacob was in love with his beautiful Rachel .\n",
            "2020-06-03 21:03:27,641 \tReference:  Mɔlebɛbo ne , ɛnee Gyekɔbo kulo Relahyɛle mɔɔ anwo yɛ ye fɛ la .\n",
            "2020-06-03 21:03:27,641 \tHypothesis: Gyekɔbo nee ɔ Rachel mɔɔ le kɛnlɛma la nyianle ɛlɔlɛ .\n",
            "2020-06-03 21:03:27,641 Validation result (greedy) at epoch  21, step    20000: bleu:  20.75, loss: 41663.0938, ppl:   5.7262, duration: 12.4695s\n",
            "2020-06-03 21:03:38,134 Epoch  21 Step:    20100 Batch Loss:     1.863874 Tokens per Sec:    20676, Lr: 0.000300\n",
            "2020-06-03 21:03:48,684 Epoch  21 Step:    20200 Batch Loss:     1.582502 Tokens per Sec:    20601, Lr: 0.000300\n",
            "2020-06-03 21:03:59,228 Epoch  21 Step:    20300 Batch Loss:     2.046744 Tokens per Sec:    20393, Lr: 0.000300\n",
            "2020-06-03 21:04:09,829 Epoch  21 Step:    20400 Batch Loss:     1.072587 Tokens per Sec:    20711, Lr: 0.000300\n",
            "2020-06-03 21:04:20,490 Epoch  21 Step:    20500 Batch Loss:     2.276052 Tokens per Sec:    20953, Lr: 0.000300\n",
            "2020-06-03 21:04:30,951 Epoch  21 Step:    20600 Batch Loss:     1.520571 Tokens per Sec:    20951, Lr: 0.000300\n",
            "2020-06-03 21:04:41,487 Epoch  21 Step:    20700 Batch Loss:     1.837185 Tokens per Sec:    20704, Lr: 0.000300\n",
            "2020-06-03 21:04:49,004 Epoch  21: total training loss 1807.89\n",
            "2020-06-03 21:04:49,004 EPOCH 22\n",
            "2020-06-03 21:04:52,105 Epoch  22 Step:    20800 Batch Loss:     1.898865 Tokens per Sec:    19439, Lr: 0.000300\n",
            "2020-06-03 21:05:02,723 Epoch  22 Step:    20900 Batch Loss:     1.796489 Tokens per Sec:    21244, Lr: 0.000300\n",
            "2020-06-03 21:05:13,240 Epoch  22 Step:    21000 Batch Loss:     1.937369 Tokens per Sec:    20796, Lr: 0.000300\n",
            "2020-06-03 21:05:26,003 Hooray! New best validation result [ppl]!\n",
            "2020-06-03 21:05:26,003 Saving new checkpoint.\n",
            "2020-06-03 21:05:26,232 Example #0\n",
            "2020-06-03 21:05:26,233 \tSource:     What motivated Moses to live as he did ?\n",
            "2020-06-03 21:05:26,233 \tReference:  Duzu ati a Mosisi kpale kɛ ɔbabɔ ye ɛbɛla kɛmɔ yɛha nwolɛ edwɛkɛ la ɛ ?\n",
            "2020-06-03 21:05:26,233 \tHypothesis: Duzu a manle Mosisi dele nganeɛ wɔ mekɛ mɔɔ ɔyɛle ye la ɛ ?\n",
            "2020-06-03 21:05:26,233 Example #1\n",
            "2020-06-03 21:05:26,233 \tSource:     Writing Committee\n",
            "2020-06-03 21:05:26,233 \tReference:  Mbulukuhɛlɛlɛ Kɔmatii\n",
            "2020-06-03 21:05:26,233 \tHypothesis: Bɛdabɛ Mɔɔ Bɛyɛ Bɛ Ndetee La\n",
            "2020-06-03 21:05:26,233 Example #2\n",
            "2020-06-03 21:05:26,234 \tSource:     He was simply expressing a realistic view of how life in this imperfect world turns out .\n",
            "2020-06-03 21:05:26,234 \tReference:  Mɔɔ kɔ zo wɔ ewiade ɛtane ɛhye anu la anwo edwɛkɛ yɛɛ ɛnee ɔlɛka a .\n",
            "2020-06-03 21:05:26,234 \tHypothesis: Ɛnee ɔtɛkele kɛzi ɛbɛlabɔlɛ mɔɔ di munli wɔ ewiade ɛhye anu la anwo .\n",
            "2020-06-03 21:05:26,234 Example #3\n",
            "2020-06-03 21:05:26,234 \tSource:     From the start , Jacob was in love with his beautiful Rachel .\n",
            "2020-06-03 21:05:26,234 \tReference:  Mɔlebɛbo ne , ɛnee Gyekɔbo kulo Relahyɛle mɔɔ anwo yɛ ye fɛ la .\n",
            "2020-06-03 21:05:26,234 \tHypothesis: Ɔvi mɔlebɛbo ne , Gyekɔbo nyianle ɔ nwo ɛlɔlɛ kpole wɔ Rachel anwo .\n",
            "2020-06-03 21:05:26,234 Validation result (greedy) at epoch  22, step    21000: bleu:  21.01, loss: 41285.9258, ppl:   5.6364, duration: 12.9938s\n",
            "2020-06-03 21:05:36,774 Epoch  22 Step:    21100 Batch Loss:     1.973979 Tokens per Sec:    20288, Lr: 0.000300\n",
            "2020-06-03 21:05:47,362 Epoch  22 Step:    21200 Batch Loss:     1.696859 Tokens per Sec:    21028, Lr: 0.000300\n",
            "2020-06-03 21:05:57,860 Epoch  22 Step:    21300 Batch Loss:     1.866471 Tokens per Sec:    20741, Lr: 0.000300\n",
            "2020-06-03 21:06:08,386 Epoch  22 Step:    21400 Batch Loss:     1.235644 Tokens per Sec:    20303, Lr: 0.000300\n",
            "2020-06-03 21:06:18,859 Epoch  22 Step:    21500 Batch Loss:     1.492761 Tokens per Sec:    20399, Lr: 0.000300\n",
            "2020-06-03 21:06:29,481 Epoch  22 Step:    21600 Batch Loss:     2.213536 Tokens per Sec:    21211, Lr: 0.000300\n",
            "2020-06-03 21:06:40,040 Epoch  22 Step:    21700 Batch Loss:     2.031514 Tokens per Sec:    20922, Lr: 0.000300\n",
            "2020-06-03 21:06:46,577 Epoch  22: total training loss 1796.88\n",
            "2020-06-03 21:06:46,577 EPOCH 23\n",
            "2020-06-03 21:06:50,679 Epoch  23 Step:    21800 Batch Loss:     2.012747 Tokens per Sec:    20632, Lr: 0.000300\n",
            "2020-06-03 21:07:01,361 Epoch  23 Step:    21900 Batch Loss:     2.413381 Tokens per Sec:    20847, Lr: 0.000300\n",
            "2020-06-03 21:07:11,870 Epoch  23 Step:    22000 Batch Loss:     1.724056 Tokens per Sec:    20731, Lr: 0.000300\n",
            "2020-06-03 21:07:24,287 Hooray! New best validation result [ppl]!\n",
            "2020-06-03 21:07:24,287 Saving new checkpoint.\n",
            "2020-06-03 21:07:24,514 Example #0\n",
            "2020-06-03 21:07:24,515 \tSource:     What motivated Moses to live as he did ?\n",
            "2020-06-03 21:07:24,515 \tReference:  Duzu ati a Mosisi kpale kɛ ɔbabɔ ye ɛbɛla kɛmɔ yɛha nwolɛ edwɛkɛ la ɛ ?\n",
            "2020-06-03 21:07:24,515 \tHypothesis: Duzu a manle Mosisi dele nganeɛ wɔ mekɛ mɔɔ ɔyɛle ye la ɛ ?\n",
            "2020-06-03 21:07:24,515 Example #1\n",
            "2020-06-03 21:07:24,515 \tSource:     Writing Committee\n",
            "2020-06-03 21:07:24,515 \tReference:  Mbulukuhɛlɛlɛ Kɔmatii\n",
            "2020-06-03 21:07:24,515 \tHypothesis: Bɛyɛ Bɛ Nwo Kpalɛ\n",
            "2020-06-03 21:07:24,515 Example #2\n",
            "2020-06-03 21:07:24,516 \tSource:     He was simply expressing a realistic view of how life in this imperfect world turns out .\n",
            "2020-06-03 21:07:24,516 \tReference:  Mɔɔ kɔ zo wɔ ewiade ɛtane ɛhye anu la anwo edwɛkɛ yɛɛ ɛnee ɔlɛka a .\n",
            "2020-06-03 21:07:24,516 \tHypothesis: Ɛnee ɔlɛ adwenle mɔɔ anu yɛ se la wɔ kɛzi ɛbɛlabɔlɛ mɔɔ ɛnli munli la anu .\n",
            "2020-06-03 21:07:24,516 Example #3\n",
            "2020-06-03 21:07:24,516 \tSource:     From the start , Jacob was in love with his beautiful Rachel .\n",
            "2020-06-03 21:07:24,516 \tReference:  Mɔlebɛbo ne , ɛnee Gyekɔbo kulo Relahyɛle mɔɔ anwo yɛ ye fɛ la .\n",
            "2020-06-03 21:07:24,516 \tHypothesis: Ɔvi mɔlebɛbo ne , ɛnee Gyekɔbo kulo ɔ nwo kpalɛ .\n",
            "2020-06-03 21:07:24,516 Validation result (greedy) at epoch  23, step    22000: bleu:  21.99, loss: 40969.8945, ppl:   5.5623, duration: 12.6459s\n",
            "2020-06-03 21:07:35,199 Epoch  23 Step:    22100 Batch Loss:     2.034916 Tokens per Sec:    20388, Lr: 0.000300\n",
            "2020-06-03 21:07:45,786 Epoch  23 Step:    22200 Batch Loss:     1.983697 Tokens per Sec:    20619, Lr: 0.000300\n",
            "2020-06-03 21:07:56,379 Epoch  23 Step:    22300 Batch Loss:     1.846369 Tokens per Sec:    21062, Lr: 0.000300\n",
            "2020-06-03 21:08:06,948 Epoch  23 Step:    22400 Batch Loss:     1.908248 Tokens per Sec:    20857, Lr: 0.000300\n",
            "2020-06-03 21:08:17,452 Epoch  23 Step:    22500 Batch Loss:     1.958023 Tokens per Sec:    20477, Lr: 0.000300\n",
            "2020-06-03 21:08:28,024 Epoch  23 Step:    22600 Batch Loss:     1.630192 Tokens per Sec:    20546, Lr: 0.000300\n",
            "2020-06-03 21:08:38,551 Epoch  23 Step:    22700 Batch Loss:     1.805249 Tokens per Sec:    20946, Lr: 0.000300\n",
            "2020-06-03 21:08:43,765 Epoch  23: total training loss 1770.19\n",
            "2020-06-03 21:08:43,765 EPOCH 24\n",
            "2020-06-03 21:08:49,350 Epoch  24 Step:    22800 Batch Loss:     2.079514 Tokens per Sec:    20608, Lr: 0.000300\n",
            "2020-06-03 21:08:59,799 Epoch  24 Step:    22900 Batch Loss:     1.696400 Tokens per Sec:    20599, Lr: 0.000300\n",
            "2020-06-03 21:09:10,333 Epoch  24 Step:    23000 Batch Loss:     1.696003 Tokens per Sec:    20492, Lr: 0.000300\n",
            "2020-06-03 21:09:24,212 Hooray! New best validation result [ppl]!\n",
            "2020-06-03 21:09:24,212 Saving new checkpoint.\n",
            "2020-06-03 21:09:24,438 Example #0\n",
            "2020-06-03 21:09:24,438 \tSource:     What motivated Moses to live as he did ?\n",
            "2020-06-03 21:09:24,438 \tReference:  Duzu ati a Mosisi kpale kɛ ɔbabɔ ye ɛbɛla kɛmɔ yɛha nwolɛ edwɛkɛ la ɛ ?\n",
            "2020-06-03 21:09:24,438 \tHypothesis: Duzu a manle Mosisi dele nganeɛ kɛ ɔyɛle a ?\n",
            "2020-06-03 21:09:24,438 Example #1\n",
            "2020-06-03 21:09:24,439 \tSource:     Writing Committee\n",
            "2020-06-03 21:09:24,439 \tReference:  Mbulukuhɛlɛlɛ Kɔmatii\n",
            "2020-06-03 21:09:24,439 \tHypothesis: Bɛdabɛ Mɔɔ Bɛyɛ Bɛ Nye La\n",
            "2020-06-03 21:09:24,439 Example #2\n",
            "2020-06-03 21:09:24,439 \tSource:     He was simply expressing a realistic view of how life in this imperfect world turns out .\n",
            "2020-06-03 21:09:24,439 \tReference:  Mɔɔ kɔ zo wɔ ewiade ɛtane ɛhye anu la anwo edwɛkɛ yɛɛ ɛnee ɔlɛka a .\n",
            "2020-06-03 21:09:24,439 \tHypothesis: Ɛnee ɔtɛwiele ye adwenle kɛ ɛbɛlabɔlɛ mɔɔ di munli wɔ ewiade ɛhye anu la anwo hyia .\n",
            "2020-06-03 21:09:24,439 Example #3\n",
            "2020-06-03 21:09:24,439 \tSource:     From the start , Jacob was in love with his beautiful Rachel .\n",
            "2020-06-03 21:09:24,439 \tReference:  Mɔlebɛbo ne , ɛnee Gyekɔbo kulo Relahyɛle mɔɔ anwo yɛ ye fɛ la .\n",
            "2020-06-03 21:09:24,440 \tHypothesis: Ɔvi mɔlebɛbo ne , ɛnee Gyekɔbo kulo ɔ gɔnwo Rachel kpalɛ .\n",
            "2020-06-03 21:09:24,440 Validation result (greedy) at epoch  24, step    23000: bleu:  21.64, loss: 40625.3320, ppl:   5.4826, duration: 14.1065s\n",
            "2020-06-03 21:09:35,017 Epoch  24 Step:    23100 Batch Loss:     2.437564 Tokens per Sec:    20582, Lr: 0.000300\n",
            "2020-06-03 21:09:45,585 Epoch  24 Step:    23200 Batch Loss:     1.509386 Tokens per Sec:    20750, Lr: 0.000300\n",
            "2020-06-03 21:09:56,049 Epoch  24 Step:    23300 Batch Loss:     2.092568 Tokens per Sec:    20627, Lr: 0.000300\n",
            "2020-06-03 21:10:06,630 Epoch  24 Step:    23400 Batch Loss:     1.654654 Tokens per Sec:    20789, Lr: 0.000300\n",
            "2020-06-03 21:10:17,208 Epoch  24 Step:    23500 Batch Loss:     1.593107 Tokens per Sec:    20746, Lr: 0.000300\n",
            "2020-06-03 21:10:27,690 Epoch  24 Step:    23600 Batch Loss:     1.759732 Tokens per Sec:    20831, Lr: 0.000300\n",
            "2020-06-03 21:10:38,250 Epoch  24 Step:    23700 Batch Loss:     1.301491 Tokens per Sec:    20766, Lr: 0.000300\n",
            "2020-06-03 21:10:42,655 Epoch  24: total training loss 1767.83\n",
            "2020-06-03 21:10:42,656 EPOCH 25\n",
            "2020-06-03 21:10:48,922 Epoch  25 Step:    23800 Batch Loss:     2.147986 Tokens per Sec:    20582, Lr: 0.000300\n",
            "2020-06-03 21:10:59,579 Epoch  25 Step:    23900 Batch Loss:     1.796987 Tokens per Sec:    20551, Lr: 0.000300\n",
            "2020-06-03 21:11:10,156 Epoch  25 Step:    24000 Batch Loss:     1.652870 Tokens per Sec:    20899, Lr: 0.000300\n",
            "2020-06-03 21:11:21,682 Hooray! New best validation result [ppl]!\n",
            "2020-06-03 21:11:21,682 Saving new checkpoint.\n",
            "2020-06-03 21:11:21,906 Example #0\n",
            "2020-06-03 21:11:21,906 \tSource:     What motivated Moses to live as he did ?\n",
            "2020-06-03 21:11:21,907 \tReference:  Duzu ati a Mosisi kpale kɛ ɔbabɔ ye ɛbɛla kɛmɔ yɛha nwolɛ edwɛkɛ la ɛ ?\n",
            "2020-06-03 21:11:21,907 \tHypothesis: Duzu a manle Mosisi dele nganeɛ wɔ mekɛ mɔɔ ɔyɛle la ɛ ?\n",
            "2020-06-03 21:11:21,907 Example #1\n",
            "2020-06-03 21:11:21,907 \tSource:     Writing Committee\n",
            "2020-06-03 21:11:21,907 \tReference:  Mbulukuhɛlɛlɛ Kɔmatii\n",
            "2020-06-03 21:11:21,907 \tHypothesis: Bɛyɛ Bɛ Nye\n",
            "2020-06-03 21:11:21,907 Example #2\n",
            "2020-06-03 21:11:21,907 \tSource:     He was simply expressing a realistic view of how life in this imperfect world turns out .\n",
            "2020-06-03 21:11:21,907 \tReference:  Mɔɔ kɔ zo wɔ ewiade ɛtane ɛhye anu la anwo edwɛkɛ yɛɛ ɛnee ɔlɛka a .\n",
            "2020-06-03 21:11:21,908 \tHypothesis: Ɛnee ɔze kɛzi ɛbɛlabɔlɛ mɔɔ di munli wɔ ewiade ɛhye anu la .\n",
            "2020-06-03 21:11:21,908 Example #3\n",
            "2020-06-03 21:11:21,908 \tSource:     From the start , Jacob was in love with his beautiful Rachel .\n",
            "2020-06-03 21:11:21,908 \tReference:  Mɔlebɛbo ne , ɛnee Gyekɔbo kulo Relahyɛle mɔɔ anwo yɛ ye fɛ la .\n",
            "2020-06-03 21:11:21,908 \tHypothesis: Ɔvi mɔlebɛbo ne , Gyekɔbo nee ɔ Rachel ɛlɔlɛ ne mɔɔ ɔ nee ye yɛle la .\n",
            "2020-06-03 21:11:21,908 Validation result (greedy) at epoch  25, step    24000: bleu:  22.17, loss: 40087.4727, ppl:   5.3605, duration: 11.7516s\n",
            "2020-06-03 21:11:32,450 Epoch  25 Step:    24100 Batch Loss:     1.593843 Tokens per Sec:    20214, Lr: 0.000300\n",
            "2020-06-03 21:11:42,989 Epoch  25 Step:    24200 Batch Loss:     2.255699 Tokens per Sec:    20532, Lr: 0.000300\n",
            "2020-06-03 21:11:53,443 Epoch  25 Step:    24300 Batch Loss:     1.371806 Tokens per Sec:    20376, Lr: 0.000300\n",
            "2020-06-03 21:12:03,995 Epoch  25 Step:    24400 Batch Loss:     2.295708 Tokens per Sec:    20726, Lr: 0.000300\n",
            "2020-06-03 21:12:14,612 Epoch  25 Step:    24500 Batch Loss:     0.931725 Tokens per Sec:    20878, Lr: 0.000300\n",
            "2020-06-03 21:12:25,286 Epoch  25 Step:    24600 Batch Loss:     1.770816 Tokens per Sec:    21046, Lr: 0.000300\n",
            "2020-06-03 21:12:35,950 Epoch  25 Step:    24700 Batch Loss:     1.652400 Tokens per Sec:    21234, Lr: 0.000300\n",
            "2020-06-03 21:12:39,255 Epoch  25: total training loss 1737.17\n",
            "2020-06-03 21:12:39,255 EPOCH 26\n",
            "2020-06-03 21:12:46,721 Epoch  26 Step:    24800 Batch Loss:     1.684442 Tokens per Sec:    20130, Lr: 0.000300\n",
            "2020-06-03 21:12:57,221 Epoch  26 Step:    24900 Batch Loss:     2.222572 Tokens per Sec:    20810, Lr: 0.000300\n",
            "2020-06-03 21:13:07,687 Epoch  26 Step:    25000 Batch Loss:     1.510064 Tokens per Sec:    20436, Lr: 0.000300\n",
            "2020-06-03 21:13:23,301 Hooray! New best validation result [ppl]!\n",
            "2020-06-03 21:13:23,301 Saving new checkpoint.\n",
            "2020-06-03 21:13:23,525 Example #0\n",
            "2020-06-03 21:13:23,525 \tSource:     What motivated Moses to live as he did ?\n",
            "2020-06-03 21:13:23,525 \tReference:  Duzu ati a Mosisi kpale kɛ ɔbabɔ ye ɛbɛla kɛmɔ yɛha nwolɛ edwɛkɛ la ɛ ?\n",
            "2020-06-03 21:13:23,525 \tHypothesis: Duzu a manle Mosisi dele nganeɛ kɛ ɔyɛle a ?\n",
            "2020-06-03 21:13:23,525 Example #1\n",
            "2020-06-03 21:13:23,525 \tSource:     Writing Committee\n",
            "2020-06-03 21:13:23,525 \tReference:  Mbulukuhɛlɛlɛ Kɔmatii\n",
            "2020-06-03 21:13:23,526 \tHypothesis: Bɛyɛ Bɛ Nye\n",
            "2020-06-03 21:13:23,526 Example #2\n",
            "2020-06-03 21:13:23,526 \tSource:     He was simply expressing a realistic view of how life in this imperfect world turns out .\n",
            "2020-06-03 21:13:23,526 \tReference:  Mɔɔ kɔ zo wɔ ewiade ɛtane ɛhye anu la anwo edwɛkɛ yɛɛ ɛnee ɔlɛka a .\n",
            "2020-06-03 21:13:23,526 \tHypothesis: Ɛnee ɔze kɛzi ngoane wɔ ewiade ɛhye anu mɔɔ ɛnli munli la anwo edwɛkɛ .\n",
            "2020-06-03 21:13:23,526 Example #3\n",
            "2020-06-03 21:13:23,526 \tSource:     From the start , Jacob was in love with his beautiful Rachel .\n",
            "2020-06-03 21:13:23,526 \tReference:  Mɔlebɛbo ne , ɛnee Gyekɔbo kulo Relahyɛle mɔɔ anwo yɛ ye fɛ la .\n",
            "2020-06-03 21:13:23,526 \tHypothesis: Ɔvi mɔlebɛbo ne , ɛnee Gyekɔbo kulo ɔ gɔnwo Rachel kpalɛ .\n",
            "2020-06-03 21:13:23,526 Validation result (greedy) at epoch  26, step    25000: bleu:  22.58, loss: 40029.9297, ppl:   5.3476, duration: 15.8389s\n",
            "2020-06-03 21:13:34,098 Epoch  26 Step:    25100 Batch Loss:     1.624580 Tokens per Sec:    20767, Lr: 0.000300\n",
            "2020-06-03 21:13:44,648 Epoch  26 Step:    25200 Batch Loss:     1.698826 Tokens per Sec:    20676, Lr: 0.000300\n",
            "2020-06-03 21:13:55,080 Epoch  26 Step:    25300 Batch Loss:     1.876090 Tokens per Sec:    20294, Lr: 0.000300\n",
            "2020-06-03 21:14:05,751 Epoch  26 Step:    25400 Batch Loss:     1.555739 Tokens per Sec:    20685, Lr: 0.000300\n",
            "2020-06-03 21:14:16,252 Epoch  26 Step:    25500 Batch Loss:     1.819406 Tokens per Sec:    20702, Lr: 0.000300\n",
            "2020-06-03 21:14:26,868 Epoch  26 Step:    25600 Batch Loss:     1.004909 Tokens per Sec:    20695, Lr: 0.000300\n",
            "2020-06-03 21:14:37,378 Epoch  26 Step:    25700 Batch Loss:     2.063580 Tokens per Sec:    21020, Lr: 0.000300\n",
            "2020-06-03 21:14:40,150 Epoch  26: total training loss 1735.56\n",
            "2020-06-03 21:14:40,150 EPOCH 27\n",
            "2020-06-03 21:14:47,931 Epoch  27 Step:    25800 Batch Loss:     1.606948 Tokens per Sec:    20244, Lr: 0.000300\n",
            "2020-06-03 21:14:58,347 Epoch  27 Step:    25900 Batch Loss:     1.683746 Tokens per Sec:    20475, Lr: 0.000300\n",
            "2020-06-03 21:15:08,878 Epoch  27 Step:    26000 Batch Loss:     1.125876 Tokens per Sec:    21175, Lr: 0.000300\n",
            "2020-06-03 21:15:20,801 Hooray! New best validation result [ppl]!\n",
            "2020-06-03 21:15:20,801 Saving new checkpoint.\n",
            "2020-06-03 21:15:21,031 Example #0\n",
            "2020-06-03 21:15:21,032 \tSource:     What motivated Moses to live as he did ?\n",
            "2020-06-03 21:15:21,032 \tReference:  Duzu ati a Mosisi kpale kɛ ɔbabɔ ye ɛbɛla kɛmɔ yɛha nwolɛ edwɛkɛ la ɛ ?\n",
            "2020-06-03 21:15:21,032 \tHypothesis: Duzu a manle Mosisi dele nganeɛ wɔ mekɛ mɔɔ ɔyɛle ye la ɛ ?\n",
            "2020-06-03 21:15:21,032 Example #1\n",
            "2020-06-03 21:15:21,032 \tSource:     Writing Committee\n",
            "2020-06-03 21:15:21,032 \tReference:  Mbulukuhɛlɛlɛ Kɔmatii\n",
            "2020-06-03 21:15:21,032 \tHypothesis: Bɛyɛ Bɛtelɛhyɛne\n",
            "2020-06-03 21:15:21,032 Example #2\n",
            "2020-06-03 21:15:21,032 \tSource:     He was simply expressing a realistic view of how life in this imperfect world turns out .\n",
            "2020-06-03 21:15:21,032 \tReference:  Mɔɔ kɔ zo wɔ ewiade ɛtane ɛhye anu la anwo edwɛkɛ yɛɛ ɛnee ɔlɛka a .\n",
            "2020-06-03 21:15:21,033 \tHypothesis: Ɛnee ɔze kɛzi ɛbɛlabɔlɛ mɔɔ di munli la wɔ ewiade ɛhye anu .\n",
            "2020-06-03 21:15:21,033 Example #3\n",
            "2020-06-03 21:15:21,033 \tSource:     From the start , Jacob was in love with his beautiful Rachel .\n",
            "2020-06-03 21:15:21,033 \tReference:  Mɔlebɛbo ne , ɛnee Gyekɔbo kulo Relahyɛle mɔɔ anwo yɛ ye fɛ la .\n",
            "2020-06-03 21:15:21,033 \tHypothesis: Ɔvi mɔlebɛbo ne , ɛnee Gyekɔbo kulo ɔ gɔnwo Rachel kɛnlɛma ne .\n",
            "2020-06-03 21:15:21,033 Validation result (greedy) at epoch  27, step    26000: bleu:  22.08, loss: 39606.6445, ppl:   5.2536, duration: 12.1545s\n",
            "2020-06-03 21:15:31,713 Epoch  27 Step:    26100 Batch Loss:     1.581473 Tokens per Sec:    20994, Lr: 0.000300\n",
            "2020-06-03 21:15:42,298 Epoch  27 Step:    26200 Batch Loss:     2.304857 Tokens per Sec:    20829, Lr: 0.000300\n",
            "2020-06-03 21:15:52,965 Epoch  27 Step:    26300 Batch Loss:     1.630884 Tokens per Sec:    20970, Lr: 0.000300\n",
            "2020-06-03 21:16:03,509 Epoch  27 Step:    26400 Batch Loss:     1.548669 Tokens per Sec:    20735, Lr: 0.000300\n",
            "2020-06-03 21:16:14,041 Epoch  27 Step:    26500 Batch Loss:     1.820281 Tokens per Sec:    20846, Lr: 0.000300\n",
            "2020-06-03 21:16:24,514 Epoch  27 Step:    26600 Batch Loss:     2.092907 Tokens per Sec:    20520, Lr: 0.000300\n",
            "2020-06-03 21:16:35,070 Epoch  27 Step:    26700 Batch Loss:     1.728870 Tokens per Sec:    21016, Lr: 0.000300\n",
            "2020-06-03 21:16:36,572 Epoch  27: total training loss 1708.65\n",
            "2020-06-03 21:16:36,573 EPOCH 28\n",
            "2020-06-03 21:16:45,753 Epoch  28 Step:    26800 Batch Loss:     1.520230 Tokens per Sec:    20583, Lr: 0.000300\n",
            "2020-06-03 21:16:56,364 Epoch  28 Step:    26900 Batch Loss:     1.751628 Tokens per Sec:    20876, Lr: 0.000300\n",
            "2020-06-03 21:17:06,989 Epoch  28 Step:    27000 Batch Loss:     1.911934 Tokens per Sec:    20781, Lr: 0.000300\n",
            "2020-06-03 21:17:18,718 Hooray! New best validation result [ppl]!\n",
            "2020-06-03 21:17:18,718 Saving new checkpoint.\n",
            "2020-06-03 21:17:18,937 Example #0\n",
            "2020-06-03 21:17:18,938 \tSource:     What motivated Moses to live as he did ?\n",
            "2020-06-03 21:17:18,938 \tReference:  Duzu ati a Mosisi kpale kɛ ɔbabɔ ye ɛbɛla kɛmɔ yɛha nwolɛ edwɛkɛ la ɛ ?\n",
            "2020-06-03 21:17:18,938 \tHypothesis: Duzu a manle Mosisi dele nganeɛ wɔ mekɛ mɔɔ ɔyɛle la ɛ ?\n",
            "2020-06-03 21:17:18,938 Example #1\n",
            "2020-06-03 21:17:18,938 \tSource:     Writing Committee\n",
            "2020-06-03 21:17:18,938 \tReference:  Mbulukuhɛlɛlɛ Kɔmatii\n",
            "2020-06-03 21:17:18,938 \tHypothesis: Bɛdabɛ Mɔɔ Bɛyɛ La\n",
            "2020-06-03 21:17:18,938 Example #2\n",
            "2020-06-03 21:17:18,938 \tSource:     He was simply expressing a realistic view of how life in this imperfect world turns out .\n",
            "2020-06-03 21:17:18,939 \tReference:  Mɔɔ kɔ zo wɔ ewiade ɛtane ɛhye anu la anwo edwɛkɛ yɛɛ ɛnee ɔlɛka a .\n",
            "2020-06-03 21:17:18,939 \tHypothesis: Ɛnee ɔze kɛzi asetɛnla mɔɔ anu yɛ se la .\n",
            "2020-06-03 21:17:18,939 Example #3\n",
            "2020-06-03 21:17:18,939 \tSource:     From the start , Jacob was in love with his beautiful Rachel .\n",
            "2020-06-03 21:17:18,939 \tReference:  Mɔlebɛbo ne , ɛnee Gyekɔbo kulo Relahyɛle mɔɔ anwo yɛ ye fɛ la .\n",
            "2020-06-03 21:17:18,939 \tHypothesis: Ɔvi mɔlebɛbo ne , ɛnee Gyekɔbo kulo Rachel kpalɛ .\n",
            "2020-06-03 21:17:18,939 Validation result (greedy) at epoch  28, step    27000: bleu:  22.36, loss: 39386.1445, ppl:   5.2053, duration: 11.9497s\n",
            "2020-06-03 21:17:29,600 Epoch  28 Step:    27100 Batch Loss:     1.987078 Tokens per Sec:    20782, Lr: 0.000300\n",
            "2020-06-03 21:17:40,094 Epoch  28 Step:    27200 Batch Loss:     1.677736 Tokens per Sec:    20649, Lr: 0.000300\n",
            "2020-06-03 21:17:50,530 Epoch  28 Step:    27300 Batch Loss:     2.312309 Tokens per Sec:    20074, Lr: 0.000300\n",
            "2020-06-03 21:18:01,171 Epoch  28 Step:    27400 Batch Loss:     1.756586 Tokens per Sec:    21005, Lr: 0.000300\n",
            "2020-06-03 21:18:11,727 Epoch  28 Step:    27500 Batch Loss:     2.005115 Tokens per Sec:    20321, Lr: 0.000300\n",
            "2020-06-03 21:18:22,307 Epoch  28 Step:    27600 Batch Loss:     1.936458 Tokens per Sec:    20754, Lr: 0.000300\n",
            "2020-06-03 21:18:32,813 Epoch  28 Step:    27700 Batch Loss:     1.615831 Tokens per Sec:    20866, Lr: 0.000300\n",
            "2020-06-03 21:18:33,399 Epoch  28: total training loss 1701.49\n",
            "2020-06-03 21:18:33,399 EPOCH 29\n",
            "2020-06-03 21:18:43,338 Epoch  29 Step:    27800 Batch Loss:     1.670728 Tokens per Sec:    20313, Lr: 0.000300\n",
            "2020-06-03 21:18:53,825 Epoch  29 Step:    27900 Batch Loss:     1.730662 Tokens per Sec:    20463, Lr: 0.000300\n",
            "2020-06-03 21:19:04,412 Epoch  29 Step:    28000 Batch Loss:     1.674679 Tokens per Sec:    20773, Lr: 0.000300\n",
            "2020-06-03 21:19:17,345 Hooray! New best validation result [ppl]!\n",
            "2020-06-03 21:19:17,346 Saving new checkpoint.\n",
            "2020-06-03 21:19:17,576 Example #0\n",
            "2020-06-03 21:19:17,577 \tSource:     What motivated Moses to live as he did ?\n",
            "2020-06-03 21:19:17,577 \tReference:  Duzu ati a Mosisi kpale kɛ ɔbabɔ ye ɛbɛla kɛmɔ yɛha nwolɛ edwɛkɛ la ɛ ?\n",
            "2020-06-03 21:19:17,577 \tHypothesis: Duzu a manle Mosisi dele nganeɛ kɛ ɔyɛle a ?\n",
            "2020-06-03 21:19:17,577 Example #1\n",
            "2020-06-03 21:19:17,577 \tSource:     Writing Committee\n",
            "2020-06-03 21:19:17,577 \tReference:  Mbulukuhɛlɛlɛ Kɔmatii\n",
            "2020-06-03 21:19:17,577 \tHypothesis: Bɛyɛ Bɛ Nye\n",
            "2020-06-03 21:19:17,577 Example #2\n",
            "2020-06-03 21:19:17,578 \tSource:     He was simply expressing a realistic view of how life in this imperfect world turns out .\n",
            "2020-06-03 21:19:17,578 \tReference:  Mɔɔ kɔ zo wɔ ewiade ɛtane ɛhye anu la anwo edwɛkɛ yɛɛ ɛnee ɔlɛka a .\n",
            "2020-06-03 21:19:17,578 \tHypothesis: Ɛnee ɔze kɛzi ngoane wɔ ewiade ɛhye anu mɔɔ sinlidɔlɛ wɔ nu la .\n",
            "2020-06-03 21:19:17,578 Example #3\n",
            "2020-06-03 21:19:17,578 \tSource:     From the start , Jacob was in love with his beautiful Rachel .\n",
            "2020-06-03 21:19:17,578 \tReference:  Mɔlebɛbo ne , ɛnee Gyekɔbo kulo Relahyɛle mɔɔ anwo yɛ ye fɛ la .\n",
            "2020-06-03 21:19:17,578 \tHypothesis: Ɔvi mɔlebɛbo ne , ɛnee Gyekɔbo kulo Rachel kpalɛ .\n",
            "2020-06-03 21:19:17,578 Validation result (greedy) at epoch  29, step    28000: bleu:  23.07, loss: 39063.4727, ppl:   5.1354, duration: 13.1657s\n",
            "2020-06-03 21:19:28,149 Epoch  29 Step:    28100 Batch Loss:     0.998123 Tokens per Sec:    20530, Lr: 0.000300\n",
            "2020-06-03 21:19:38,807 Epoch  29 Step:    28200 Batch Loss:     2.048493 Tokens per Sec:    21081, Lr: 0.000300\n",
            "2020-06-03 21:19:49,343 Epoch  29 Step:    28300 Batch Loss:     1.398975 Tokens per Sec:    20758, Lr: 0.000300\n",
            "2020-06-03 21:19:59,913 Epoch  29 Step:    28400 Batch Loss:     1.484908 Tokens per Sec:    20779, Lr: 0.000300\n",
            "2020-06-03 21:20:10,438 Epoch  29 Step:    28500 Batch Loss:     1.734071 Tokens per Sec:    20714, Lr: 0.000300\n",
            "2020-06-03 21:20:21,011 Epoch  29 Step:    28600 Batch Loss:     1.587435 Tokens per Sec:    20897, Lr: 0.000300\n",
            "2020-06-03 21:20:31,407 Epoch  29: total training loss 1685.91\n",
            "2020-06-03 21:20:31,407 EPOCH 30\n",
            "2020-06-03 21:20:31,620 Epoch  30 Step:    28700 Batch Loss:     1.584658 Tokens per Sec:    11659, Lr: 0.000300\n",
            "2020-06-03 21:20:42,220 Epoch  30 Step:    28800 Batch Loss:     1.540751 Tokens per Sec:    20862, Lr: 0.000300\n",
            "2020-06-03 21:20:52,818 Epoch  30 Step:    28900 Batch Loss:     1.498051 Tokens per Sec:    20535, Lr: 0.000300\n",
            "2020-06-03 21:21:03,431 Epoch  30 Step:    29000 Batch Loss:     1.786739 Tokens per Sec:    20946, Lr: 0.000300\n",
            "2020-06-03 21:21:15,697 Hooray! New best validation result [ppl]!\n",
            "2020-06-03 21:21:15,697 Saving new checkpoint.\n",
            "2020-06-03 21:21:15,920 Example #0\n",
            "2020-06-03 21:21:15,921 \tSource:     What motivated Moses to live as he did ?\n",
            "2020-06-03 21:21:15,921 \tReference:  Duzu ati a Mosisi kpale kɛ ɔbabɔ ye ɛbɛla kɛmɔ yɛha nwolɛ edwɛkɛ la ɛ ?\n",
            "2020-06-03 21:21:15,921 \tHypothesis: Duzu a manle Mosisi dele nganeɛ kɛ ɔyɛle a ?\n",
            "2020-06-03 21:21:15,921 Example #1\n",
            "2020-06-03 21:21:15,921 \tSource:     Writing Committee\n",
            "2020-06-03 21:21:15,921 \tReference:  Mbulukuhɛlɛlɛ Kɔmatii\n",
            "2020-06-03 21:21:15,921 \tHypothesis: Bɛyɛ Bɛ Nye\n",
            "2020-06-03 21:21:15,921 Example #2\n",
            "2020-06-03 21:21:15,921 \tSource:     He was simply expressing a realistic view of how life in this imperfect world turns out .\n",
            "2020-06-03 21:21:15,922 \tReference:  Mɔɔ kɔ zo wɔ ewiade ɛtane ɛhye anu la anwo edwɛkɛ yɛɛ ɛnee ɔlɛka a .\n",
            "2020-06-03 21:21:15,922 \tHypothesis: Ɛnee ɔze kɛzi asetɛnla mɔɔ anu yɛ se wɔ ewiade ɛhye anu la .\n",
            "2020-06-03 21:21:15,922 Example #3\n",
            "2020-06-03 21:21:15,922 \tSource:     From the start , Jacob was in love with his beautiful Rachel .\n",
            "2020-06-03 21:21:15,922 \tReference:  Mɔlebɛbo ne , ɛnee Gyekɔbo kulo Relahyɛle mɔɔ anwo yɛ ye fɛ la .\n",
            "2020-06-03 21:21:15,922 \tHypothesis: Ɔvi mɔlebɛbo ne , Gyekɔbo nee ɔ gɔnwo Rachel mɔɔ le kɛnlɛma la nyianle ɛlɔlɛ .\n",
            "2020-06-03 21:21:15,922 Validation result (greedy) at epoch  30, step    29000: bleu:  22.77, loss: 39027.2891, ppl:   5.1277, duration: 12.4910s\n",
            "2020-06-03 21:21:26,506 Epoch  30 Step:    29100 Batch Loss:     1.826788 Tokens per Sec:    20356, Lr: 0.000300\n",
            "2020-06-03 21:21:37,069 Epoch  30 Step:    29200 Batch Loss:     1.657455 Tokens per Sec:    21015, Lr: 0.000300\n",
            "2020-06-03 21:21:47,659 Epoch  30 Step:    29300 Batch Loss:     1.652403 Tokens per Sec:    20553, Lr: 0.000300\n",
            "2020-06-03 21:21:58,156 Epoch  30 Step:    29400 Batch Loss:     1.342581 Tokens per Sec:    20751, Lr: 0.000300\n",
            "2020-06-03 21:22:08,719 Epoch  30 Step:    29500 Batch Loss:     2.043320 Tokens per Sec:    20707, Lr: 0.000300\n",
            "2020-06-03 21:22:19,353 Epoch  30 Step:    29600 Batch Loss:     2.095335 Tokens per Sec:    21029, Lr: 0.000300\n",
            "2020-06-03 21:22:28,511 Epoch  30: total training loss 1669.05\n",
            "2020-06-03 21:22:28,511 Training ended after  30 epochs.\n",
            "2020-06-03 21:22:28,511 Best validation result (greedy) at step    29000:   5.13 ppl.\n",
            "/pytorch/aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n",
            "2020-06-03 21:22:44,342  dev bleu:  23.08 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
            "2020-06-03 21:22:44,342 Translations saved to: models/ennzi_transformer/00029000.hyps.dev\n",
            "2020-06-03 21:23:13,715 test bleu:  35.67 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
            "2020-06-03 21:23:13,717 Translations saved to: models/ennzi_transformer/00029000.hyps.test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MBoDS09JM807",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "4c5bf20b-9d7f-4359-eda8-e43add337731"
      },
      "source": [
        "# Copy the created models from the notebook storage to google drive for persistant storage \n",
        "!cp -r joeynmt/models/${src}${tgt}_transformer/* \"$gdrive_path/models/${src}${tgt}_transformer/\""
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: cannot create symbolic link '/content/drive/My Drive/masakhane/en-nzi-baseline/models/ennzi_transformer/best.ckpt': Operation not supported\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n94wlrCjVc17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "e0679a33-e4a8-47ac-fbcf-287227496c04"
      },
      "source": [
        "# Output our validation accuracy\n",
        "! cat \"$gdrive_path/models/${src}${tgt}_transformer/validations.txt\""
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Steps: 1000\tLoss: 89169.74219\tPPL: 41.88209\tbleu: 1.48177\tLR: 0.00030000\t*\n",
            "Steps: 2000\tLoss: 75706.90625\tPPL: 23.83059\tbleu: 3.73378\tLR: 0.00030000\t*\n",
            "Steps: 3000\tLoss: 67883.83594\tPPL: 17.17237\tbleu: 5.97375\tLR: 0.00030000\t*\n",
            "Steps: 4000\tLoss: 62694.38281\tPPL: 13.81759\tbleu: 7.82934\tLR: 0.00030000\t*\n",
            "Steps: 5000\tLoss: 59083.41016\tPPL: 11.87812\tbleu: 9.63010\tLR: 0.00030000\t*\n",
            "Steps: 6000\tLoss: 56475.47656\tPPL: 10.64899\tbleu: 11.37133\tLR: 0.00030000\t*\n",
            "Steps: 7000\tLoss: 53976.70703\tPPL: 9.59080\tbleu: 12.21892\tLR: 0.00030000\t*\n",
            "Steps: 8000\tLoss: 51988.85547\tPPL: 8.82460\tbleu: 13.26454\tLR: 0.00030000\t*\n",
            "Steps: 9000\tLoss: 50456.08594\tPPL: 8.27587\tbleu: 14.35215\tLR: 0.00030000\t*\n",
            "Steps: 10000\tLoss: 49320.16406\tPPL: 7.89134\tbleu: 15.40513\tLR: 0.00030000\t*\n",
            "Steps: 11000\tLoss: 47784.85156\tPPL: 7.39985\tbleu: 16.26002\tLR: 0.00030000\t*\n",
            "Steps: 12000\tLoss: 46769.14453\tPPL: 7.09164\tbleu: 17.11527\tLR: 0.00030000\t*\n",
            "Steps: 13000\tLoss: 45831.42969\tPPL: 6.81851\tbleu: 17.96327\tLR: 0.00030000\t*\n",
            "Steps: 14000\tLoss: 45035.70703\tPPL: 6.59500\tbleu: 18.06152\tLR: 0.00030000\t*\n",
            "Steps: 15000\tLoss: 44286.42578\tPPL: 6.39124\tbleu: 18.90987\tLR: 0.00030000\t*\n",
            "Steps: 16000\tLoss: 43750.07422\tPPL: 6.24927\tbleu: 19.50512\tLR: 0.00030000\t*\n",
            "Steps: 17000\tLoss: 43164.36328\tPPL: 6.09782\tbleu: 20.00763\tLR: 0.00030000\t*\n",
            "Steps: 18000\tLoss: 42628.82422\tPPL: 5.96256\tbleu: 20.04571\tLR: 0.00030000\t*\n",
            "Steps: 19000\tLoss: 42029.85547\tPPL: 5.81484\tbleu: 21.02570\tLR: 0.00030000\t*\n",
            "Steps: 20000\tLoss: 41663.09375\tPPL: 5.72619\tbleu: 20.75130\tLR: 0.00030000\t*\n",
            "Steps: 21000\tLoss: 41285.92578\tPPL: 5.63645\tbleu: 21.01187\tLR: 0.00030000\t*\n",
            "Steps: 22000\tLoss: 40969.89453\tPPL: 5.56233\tbleu: 21.99443\tLR: 0.00030000\t*\n",
            "Steps: 23000\tLoss: 40625.33203\tPPL: 5.48263\tbleu: 21.64381\tLR: 0.00030000\t*\n",
            "Steps: 24000\tLoss: 40087.47266\tPPL: 5.36050\tbleu: 22.16646\tLR: 0.00030000\t*\n",
            "Steps: 25000\tLoss: 40029.92969\tPPL: 5.34759\tbleu: 22.57914\tLR: 0.00030000\t*\n",
            "Steps: 26000\tLoss: 39606.64453\tPPL: 5.25362\tbleu: 22.07690\tLR: 0.00030000\t*\n",
            "Steps: 27000\tLoss: 39386.14453\tPPL: 5.20532\tbleu: 22.35748\tLR: 0.00030000\t*\n",
            "Steps: 28000\tLoss: 39063.47266\tPPL: 5.13544\tbleu: 23.06792\tLR: 0.00030000\t*\n",
            "Steps: 29000\tLoss: 39027.28906\tPPL: 5.12767\tbleu: 22.76715\tLR: 0.00030000\t*\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "66WhRE9lIhoD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "2c90dab3-f46b-4f27-fa25-d73af3bbf7a8"
      },
      "source": [
        "# Test our model\n",
        "! cd joeynmt; python3 -m joeynmt test \"$gdrive_path/models/${src}${tgt}_transformer/config.yaml\""
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-03 21:26:06,674 Hello! This is Joey-NMT.\n",
            "/pytorch/aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n",
            "2020-06-03 21:26:24,909  dev bleu:  23.08 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
            "2020-06-03 21:26:53,833 test bleu:  35.67 [Beam search decoding with beam size = 5 and alpha = 1.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHqfci3Pvp3w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}